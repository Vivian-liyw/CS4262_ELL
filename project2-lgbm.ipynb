{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Rescaling\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split as tts\n\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:03:20.636684Z","iopub.execute_input":"2022-11-22T01:03:20.637412Z","iopub.status.idle":"2022-11-22T01:03:30.802164Z","shell.execute_reply.started":"2022-11-22T01:03:20.637352Z","shell.execute_reply":"2022-11-22T01:03:30.800759Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"seed = 0\n\n# load training and testing data\ntrain_df = pd.read_csv(\"../input/feedback-prize-english-language-learning/train.csv\", index_col=\"text_id\")\nX_train = train_df['full_text']\ncols = [col for col in train_df.columns if col != \"full_text\"]\ny_train = train_df[cols]\n\nprint(X_train[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:04:18.914430Z","iopub.execute_input":"2022-11-22T01:04:18.914907Z","iopub.status.idle":"2022-11-22T01:04:19.171801Z","shell.execute_reply.started":"2022-11-22T01:04:18.914868Z","shell.execute_reply":"2022-11-22T01:04:19.170475Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\n\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\n\nmost students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.\n\nwhen your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class.              \n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/feedback-prize-english-language-learning/test.csv\", index_col=\"text_id\")\nX_test = test_df['full_text']\nX_test_idx = X_test.index\n\nprint(X_test_idx)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:04:21.210306Z","iopub.execute_input":"2022-11-22T01:04:21.210898Z","iopub.status.idle":"2022-11-22T01:04:21.228597Z","shell.execute_reply.started":"2022-11-22T01:04:21.210846Z","shell.execute_reply":"2022-11-22T01:04:21.227580Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Index(['0000C359D63E', '000BAD50D026', '00367BB2546B'], dtype='object', name='text_id')\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean the punctuation within the text\nX_train = X_train.replace(re.compile(r'[\\n\\t]'), ' ', regex=True)\nX_train = X_train.replace(re.compile(r'[^\\w\\s]'), '', regex=True)\n\nprint(X_train[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:04:24.006691Z","iopub.execute_input":"2022-11-22T01:04:24.007161Z","iopub.status.idle":"2022-11-22T01:04:24.256165Z","shell.execute_reply.started":"2022-11-22T01:04:24.007123Z","shell.execute_reply":"2022-11-22T01:04:24.254587Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"I think that students would benefit from learning at homebecause they wont have to change and get up early in the morning to shower and do there hair taking only classes helps them because at there house theyll be pay more attention they will be comfortable at home  The hardest part of school is getting ready you wake up go brush your teeth and go to your closet and look at your cloths after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain Then youll have to change with the online classes you can wear anything and stay home and you wont need to stress about what to wear  most students usually take showers before school they either take it before they sleep or when they wake up some students do both to smell good that causes them do miss the bus and effects on there lesson time cause they come late to school when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go  when your home your comfortable and you pay attention it gives then an advantage to be smarter and even pass there classmates on class work public schools are difficult even if you try some teacher dont know how to teach it in then way that students understand it that causes students to fail and they may repeat the class              \n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_matrix(X_train, \"tfidf\")\nX_test = tokenizer.texts_to_matrix(X_test, \"tfidf\")\n\npca = PCA(n_components=100, whiten=True, random_state=seed)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nX_train, X_val, y_train, y_val = tts(X_train, y_train, test_size=.1, random_state=seed)\n\nlgb_trains = {}\nlgb_vals = {}\nfor col in cols:\n    exec(f\"lgb_trains['{col}'] = lgb.Dataset(X_train, y_train.{col})\")\n    exec(f\"lgb_vals['{col}'] = lgb.Dataset(X_val, y_val.{col})\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:04:26.303416Z","iopub.execute_input":"2022-11-22T01:04:26.303929Z","iopub.status.idle":"2022-11-22T01:04:46.209226Z","shell.execute_reply.started":"2022-11-22T01:04:26.303886Z","shell.execute_reply":"2022-11-22T01:04:46.207241Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@tf.autograph.experimental.do_not_convert\ndef MCRMSE_keras(y_true, y_pred):\n    return tf.reduce_mean(tf.reduce_mean(tf.square(y_true - y_pred), axis=1))\n\ndef MCRMSE_lgb(preds, eval_data):\n    diff = eval_data - preds\n    sq = np.square(diff)\n    rmse = np.sum(sq, axis=0) / eval_data.shape[0]\n    return \"MCRMSE\", np.sum(rmse) / eval_data.shape[1], False","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:04:51.223771Z","iopub.execute_input":"2022-11-22T01:04:51.224252Z","iopub.status.idle":"2022-11-22T01:04:51.232519Z","shell.execute_reply.started":"2022-11-22T01:04:51.224214Z","shell.execute_reply":"2022-11-22T01:04:51.231037Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"keras1_model = Sequential()\nkeras1_model.add(Dense(500, input_dim=X_train.shape[1], activation=\"relu\"))\nkeras1_model.add(BatchNormalization())\nkeras1_model.add(Dense(500, activation=\"relu\"))\nkeras1_model.add(Dropout(.3))\nkeras1_model.add(Dense(500, activation=LeakyReLU(.1)))\nkeras1_model.add(Dropout(.2))\nkeras1_model.add(Dense(500, activation=\"relu\"))\nkeras1_model.add(Dense(y_train.shape[1], activation=\"sigmoid\"))\nkeras1_model.add(Rescaling(4, offset=1))\n\noptimizer = optimizers.Adam(amsgrad=True)\nkeras1_model.compile(loss=MCRMSE_keras, optimizer=optimizer, metrics=[MCRMSE_keras])\nkeras1_model.fit(X_train, y_train, batch_size=2**3, epochs=30, verbose=1,\n          validation_data=(X_val, y_val), workers=30, use_multiprocessing=True,\n          callbacks=[EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)])\n\nkeras1_pred = pd.DataFrame(keras1_model.predict(X_test), columns=cols, index=X_test_idx)\nkeras1_pred","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:04:57.954781Z","iopub.execute_input":"2022-11-22T01:04:57.955232Z","iopub.status.idle":"2022-11-22T01:06:59.846608Z","shell.execute_reply.started":"2022-11-22T01:04:57.955198Z","shell.execute_reply":"2022-11-22T01:06:59.845066Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2022-11-22 01:04:58.224242: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n440/440 [==============================] - 5s 10ms/step - loss: 0.3991 - MCRMSE_keras: 0.3992 - val_loss: 0.3568 - val_MCRMSE_keras: 0.3568\nEpoch 2/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.3161 - MCRMSE_keras: 0.3161 - val_loss: 0.3332 - val_MCRMSE_keras: 0.3332\nEpoch 3/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.2864 - MCRMSE_keras: 0.2864 - val_loss: 0.3484 - val_MCRMSE_keras: 0.3484\nEpoch 4/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.2723 - MCRMSE_keras: 0.2723 - val_loss: 0.3417 - val_MCRMSE_keras: 0.3417\nEpoch 5/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.2536 - MCRMSE_keras: 0.2536 - val_loss: 0.3424 - val_MCRMSE_keras: 0.3424\nEpoch 6/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.2346 - MCRMSE_keras: 0.2346 - val_loss: 0.3253 - val_MCRMSE_keras: 0.3253\nEpoch 7/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.2227 - MCRMSE_keras: 0.2227 - val_loss: 0.3441 - val_MCRMSE_keras: 0.3441\nEpoch 8/30\n440/440 [==============================] - 4s 10ms/step - loss: 0.2141 - MCRMSE_keras: 0.2141 - val_loss: 0.3468 - val_MCRMSE_keras: 0.3468\nEpoch 9/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.2051 - MCRMSE_keras: 0.2051 - val_loss: 0.3434 - val_MCRMSE_keras: 0.3434\nEpoch 10/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1914 - MCRMSE_keras: 0.1914 - val_loss: 0.3486 - val_MCRMSE_keras: 0.3486\nEpoch 11/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1884 - MCRMSE_keras: 0.1884 - val_loss: 0.3681 - val_MCRMSE_keras: 0.3681\nEpoch 12/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1841 - MCRMSE_keras: 0.1841 - val_loss: 0.3558 - val_MCRMSE_keras: 0.3558\nEpoch 13/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1769 - MCRMSE_keras: 0.1769 - val_loss: 0.3574 - val_MCRMSE_keras: 0.3574\nEpoch 14/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1715 - MCRMSE_keras: 0.1715 - val_loss: 0.3565 - val_MCRMSE_keras: 0.3565\nEpoch 15/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1622 - MCRMSE_keras: 0.1622 - val_loss: 0.3669 - val_MCRMSE_keras: 0.3669\nEpoch 16/30\n440/440 [==============================] - 4s 10ms/step - loss: 0.1619 - MCRMSE_keras: 0.1619 - val_loss: 0.3616 - val_MCRMSE_keras: 0.3616\nEpoch 17/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1595 - MCRMSE_keras: 0.1595 - val_loss: 0.3641 - val_MCRMSE_keras: 0.3641\nEpoch 18/30\n440/440 [==============================] - 4s 10ms/step - loss: 0.1560 - MCRMSE_keras: 0.1560 - val_loss: 0.3560 - val_MCRMSE_keras: 0.3560\nEpoch 19/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1525 - MCRMSE_keras: 0.1525 - val_loss: 0.3616 - val_MCRMSE_keras: 0.3616\nEpoch 20/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1475 - MCRMSE_keras: 0.1475 - val_loss: 0.3701 - val_MCRMSE_keras: 0.3701\nEpoch 21/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1423 - MCRMSE_keras: 0.1423 - val_loss: 0.3752 - val_MCRMSE_keras: 0.3752\nEpoch 22/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1421 - MCRMSE_keras: 0.1421 - val_loss: 0.3629 - val_MCRMSE_keras: 0.3629\nEpoch 23/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1413 - MCRMSE_keras: 0.1413 - val_loss: 0.3778 - val_MCRMSE_keras: 0.3778\nEpoch 24/30\n440/440 [==============================] - 4s 10ms/step - loss: 0.1337 - MCRMSE_keras: 0.1337 - val_loss: 0.3634 - val_MCRMSE_keras: 0.3634\nEpoch 25/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1353 - MCRMSE_keras: 0.1353 - val_loss: 0.3588 - val_MCRMSE_keras: 0.3588\nEpoch 26/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1319 - MCRMSE_keras: 0.1319 - val_loss: 0.3593 - val_MCRMSE_keras: 0.3593\nEpoch 27/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1293 - MCRMSE_keras: 0.1293 - val_loss: 0.3703 - val_MCRMSE_keras: 0.3703\nEpoch 28/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1280 - MCRMSE_keras: 0.1280 - val_loss: 0.3653 - val_MCRMSE_keras: 0.3653\nEpoch 29/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1245 - MCRMSE_keras: 0.1245 - val_loss: 0.3653 - val_MCRMSE_keras: 0.3653\nEpoch 30/30\n440/440 [==============================] - 4s 9ms/step - loss: 0.1207 - MCRMSE_keras: 0.1207 - val_loss: 0.3599 - val_MCRMSE_keras: 0.3599\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"              cohesion    syntax  vocabulary  phraseology   grammar  \\\ntext_id                                                               \n0000C359D63E  3.222412  3.024141    3.204347     3.079329  2.683869   \n000BAD50D026  2.909045  2.680854    2.884666     2.703308  2.446693   \n00367BB2546B  3.587918  3.540501    3.697871     3.719030  3.489804   \n\n              conventions  \ntext_id                    \n0000C359D63E     2.933691  \n000BAD50D026     2.912002  \n00367BB2546B     3.619429  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n    <tr>\n      <th>text_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0000C359D63E</th>\n      <td>3.222412</td>\n      <td>3.024141</td>\n      <td>3.204347</td>\n      <td>3.079329</td>\n      <td>2.683869</td>\n      <td>2.933691</td>\n    </tr>\n    <tr>\n      <th>000BAD50D026</th>\n      <td>2.909045</td>\n      <td>2.680854</td>\n      <td>2.884666</td>\n      <td>2.703308</td>\n      <td>2.446693</td>\n      <td>2.912002</td>\n    </tr>\n    <tr>\n      <th>00367BB2546B</th>\n      <td>3.587918</td>\n      <td>3.540501</td>\n      <td>3.697871</td>\n      <td>3.719030</td>\n      <td>3.489804</td>\n      <td>3.619429</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"keras2_model = Sequential()\nkeras2_model.add(Dense(2000, input_dim=X_train.shape[1], activation=\"relu\"))\nkeras2_model.add(BatchNormalization())\nkeras2_model.add(Dense(2000, activation=\"relu\"))\nkeras2_model.add(Dropout(.3))\nkeras2_model.add(Dense(3000, activation=LeakyReLU(.1)))\nkeras2_model.add(Dropout(.2))\nkeras2_model.add(Dense(2000, activation=\"relu\"))\nkeras2_model.add(Dense(500, activation=\"relu\"))\nkeras2_model.add(Dense(3000, activation=\"softplus\"))\nkeras2_model.add(BatchNormalization())\nkeras2_model.add(Dense(1000, activation=LeakyReLU(.1)))\nkeras2_model.add(Dropout(.3))\nkeras2_model.add(Dense(3000, activation=\"softsign\"))\nkeras2_model.add(Dense(1000, activation=LeakyReLU(.1)))\nkeras2_model.add(Dropout(.1))\nkeras2_model.add(Dense(3000, activation=\"softplus\"))\nkeras2_model.add(Dropout(.4))\nkeras2_model.add(Dense(3000, activation=\"relu\"))\nkeras2_model.add(Dense(1000, activation=\"relu\"))\nkeras2_model.add(BatchNormalization())\nkeras2_model.add(Dense(y_train.shape[1], activation=\"sigmoid\"))\nkeras2_model.add(Rescaling(4, offset=1))\n\noptimizer = optimizers.Adam(amsgrad=True)\nkeras2_model.compile(loss=MCRMSE_keras, optimizer=optimizer, metrics=[MCRMSE_keras])\nkeras2_model.fit(X_train, y_train, batch_size=2**3, epochs=40, verbose=1,\n          validation_data=(X_val, y_val), workers=30, use_multiprocessing=True,\n          callbacks=[EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)])\n\nkeras2_pred = pd.DataFrame(keras2_model.predict(X_test), columns=cols, index=X_test_idx)\nkeras2_pred","metadata":{"execution":{"iopub.status.busy":"2022-11-22T01:06:59.849524Z","iopub.execute_input":"2022-11-22T01:06:59.850875Z","iopub.status.idle":"2022-11-22T01:47:24.973894Z","shell.execute_reply.started":"2022-11-22T01:06:59.850795Z","shell.execute_reply":"2022-11-22T01:47:24.972481Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/40\n440/440 [==============================] - 77s 170ms/step - loss: 0.5026 - MCRMSE_keras: 0.5026 - val_loss: 0.4337 - val_MCRMSE_keras: 0.4337\nEpoch 2/40\n440/440 [==============================] - 74s 169ms/step - loss: 0.3811 - MCRMSE_keras: 0.3811 - val_loss: 0.3795 - val_MCRMSE_keras: 0.3795\nEpoch 3/40\n440/440 [==============================] - 82s 186ms/step - loss: 0.3683 - MCRMSE_keras: 0.3684 - val_loss: 0.4027 - val_MCRMSE_keras: 0.4027\nEpoch 4/40\n440/440 [==============================] - 78s 178ms/step - loss: 0.3482 - MCRMSE_keras: 0.3482 - val_loss: 0.3715 - val_MCRMSE_keras: 0.3715\nEpoch 5/40\n440/440 [==============================] - 78s 176ms/step - loss: 0.3426 - MCRMSE_keras: 0.3427 - val_loss: 0.4293 - val_MCRMSE_keras: 0.4293\nEpoch 6/40\n440/440 [==============================] - 78s 177ms/step - loss: 0.3369 - MCRMSE_keras: 0.3369 - val_loss: 0.4126 - val_MCRMSE_keras: 0.4126\nEpoch 7/40\n440/440 [==============================] - 77s 174ms/step - loss: 0.3460 - MCRMSE_keras: 0.3459 - val_loss: 0.3731 - val_MCRMSE_keras: 0.3731\nEpoch 8/40\n440/440 [==============================] - 75s 171ms/step - loss: 0.3207 - MCRMSE_keras: 0.3207 - val_loss: 0.3751 - val_MCRMSE_keras: 0.3751\nEpoch 9/40\n440/440 [==============================] - 74s 169ms/step - loss: 0.3183 - MCRMSE_keras: 0.3184 - val_loss: 0.3453 - val_MCRMSE_keras: 0.3453\nEpoch 10/40\n440/440 [==============================] - 76s 172ms/step - loss: 0.3157 - MCRMSE_keras: 0.3157 - val_loss: 0.3888 - val_MCRMSE_keras: 0.3888\nEpoch 11/40\n440/440 [==============================] - 75s 170ms/step - loss: 0.3086 - MCRMSE_keras: 0.3086 - val_loss: 0.3588 - val_MCRMSE_keras: 0.3588\nEpoch 12/40\n440/440 [==============================] - 72s 164ms/step - loss: 0.2907 - MCRMSE_keras: 0.2907 - val_loss: 0.3988 - val_MCRMSE_keras: 0.3988\nEpoch 13/40\n440/440 [==============================] - 71s 162ms/step - loss: 0.2801 - MCRMSE_keras: 0.2800 - val_loss: 0.3667 - val_MCRMSE_keras: 0.3667\nEpoch 14/40\n440/440 [==============================] - 71s 162ms/step - loss: 0.2690 - MCRMSE_keras: 0.2691 - val_loss: 0.3630 - val_MCRMSE_keras: 0.3630\nEpoch 15/40\n440/440 [==============================] - 72s 164ms/step - loss: 0.2604 - MCRMSE_keras: 0.2604 - val_loss: 0.3640 - val_MCRMSE_keras: 0.3640\nEpoch 16/40\n440/440 [==============================] - 74s 168ms/step - loss: 0.2688 - MCRMSE_keras: 0.2688 - val_loss: 0.3774 - val_MCRMSE_keras: 0.3774\nEpoch 17/40\n440/440 [==============================] - 73s 166ms/step - loss: 0.2756 - MCRMSE_keras: 0.2756 - val_loss: 0.4074 - val_MCRMSE_keras: 0.4074\nEpoch 18/40\n440/440 [==============================] - 74s 167ms/step - loss: 0.2474 - MCRMSE_keras: 0.2474 - val_loss: 0.3546 - val_MCRMSE_keras: 0.3546\nEpoch 19/40\n440/440 [==============================] - 75s 171ms/step - loss: 0.2518 - MCRMSE_keras: 0.2518 - val_loss: 0.3768 - val_MCRMSE_keras: 0.3768\nEpoch 20/40\n440/440 [==============================] - 73s 165ms/step - loss: 0.2515 - MCRMSE_keras: 0.2515 - val_loss: 0.3904 - val_MCRMSE_keras: 0.3904\nEpoch 21/40\n440/440 [==============================] - 71s 162ms/step - loss: 0.2434 - MCRMSE_keras: 0.2434 - val_loss: 0.3582 - val_MCRMSE_keras: 0.3582\nEpoch 22/40\n440/440 [==============================] - 73s 165ms/step - loss: 0.2380 - MCRMSE_keras: 0.2379 - val_loss: 0.3454 - val_MCRMSE_keras: 0.3454\nEpoch 23/40\n440/440 [==============================] - 71s 162ms/step - loss: 0.2244 - MCRMSE_keras: 0.2244 - val_loss: 0.3722 - val_MCRMSE_keras: 0.3722\nEpoch 24/40\n440/440 [==============================] - 72s 163ms/step - loss: 0.2227 - MCRMSE_keras: 0.2227 - val_loss: 0.3469 - val_MCRMSE_keras: 0.3469\nEpoch 25/40\n440/440 [==============================] - 72s 163ms/step - loss: 0.2204 - MCRMSE_keras: 0.2204 - val_loss: 0.3813 - val_MCRMSE_keras: 0.3813\nEpoch 26/40\n440/440 [==============================] - 73s 166ms/step - loss: 0.2174 - MCRMSE_keras: 0.2173 - val_loss: 0.3482 - val_MCRMSE_keras: 0.3482\nEpoch 27/40\n440/440 [==============================] - 75s 169ms/step - loss: 0.2163 - MCRMSE_keras: 0.2163 - val_loss: 0.3566 - val_MCRMSE_keras: 0.3566\nEpoch 28/40\n440/440 [==============================] - 73s 167ms/step - loss: 0.2149 - MCRMSE_keras: 0.2150 - val_loss: 0.3583 - val_MCRMSE_keras: 0.3583\nEpoch 29/40\n440/440 [==============================] - 75s 169ms/step - loss: 0.2144 - MCRMSE_keras: 0.2144 - val_loss: 0.3548 - val_MCRMSE_keras: 0.3548\nEpoch 30/40\n440/440 [==============================] - 77s 176ms/step - loss: 0.2153 - MCRMSE_keras: 0.2153 - val_loss: 0.3506 - val_MCRMSE_keras: 0.3506\nEpoch 31/40\n440/440 [==============================] - 78s 178ms/step - loss: 0.2161 - MCRMSE_keras: 0.2161 - val_loss: 0.3495 - val_MCRMSE_keras: 0.3495\nEpoch 32/40\n440/440 [==============================] - 77s 175ms/step - loss: 0.2166 - MCRMSE_keras: 0.2166 - val_loss: 0.3555 - val_MCRMSE_keras: 0.3555\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"              cohesion    syntax  vocabulary  phraseology   grammar  \\\ntext_id                                                               \n0000C359D63E  3.071095  2.970881    3.251100     3.109678  3.054170   \n000BAD50D026  2.720232  2.673352    3.069246     2.787282  2.762738   \n00367BB2546B  4.050164  3.804771    3.698727     3.818188  3.535031   \n\n              conventions  \ntext_id                    \n0000C359D63E     3.099910  \n000BAD50D026     2.811052  \n00367BB2546B     3.749262  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n    <tr>\n      <th>text_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0000C359D63E</th>\n      <td>3.071095</td>\n      <td>2.970881</td>\n      <td>3.251100</td>\n      <td>3.109678</td>\n      <td>3.054170</td>\n      <td>3.099910</td>\n    </tr>\n    <tr>\n      <th>000BAD50D026</th>\n      <td>2.720232</td>\n      <td>2.673352</td>\n      <td>3.069246</td>\n      <td>2.787282</td>\n      <td>2.762738</td>\n      <td>2.811052</td>\n    </tr>\n    <tr>\n      <th>00367BB2546B</th>\n      <td>4.050164</td>\n      <td>3.804771</td>\n      <td>3.698727</td>\n      <td>3.818188</td>\n      <td>3.535031</td>\n      <td>3.749262</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"lgb1_models = {}\nlgb1_preds = {}\n\nfor score in cols:\n    lgb1_params = {'objective': 'regression',\n                   'metric': 'rmse',\n                   'verbosity': 0,\n                   'early_stopping_round': 50,\n                   'random_state': seed,\n                   }\n    \n    train_set=lgb_trains[score]\n    valid_sets=lgb_vals[score]\n\n    lgb1_model = lgb.train(\n        params=lgb1_params,\n        train_set=train_set,\n        num_boost_round=1000,\n        valid_sets=(train_set, valid_sets),\n        callbacks=None,\n        verbose_eval=100\n    )\n    \n    lgb1_models[score] = lgb1_model\n    lgb1_preds[score] = lgb1_model.predict(X_test)\n    \nlgb1_pred = pd.DataFrame(lgb1_preds, index=X_test_idx)\nlgb1_pred","metadata":{"execution":{"iopub.status.busy":"2022-11-21T22:45:16.986566Z","iopub.execute_input":"2022-11-21T22:45:16.987123Z","iopub.status.idle":"2022-11-21T22:45:24.807574Z","shell.execute_reply.started":"2022-11-21T22:45:16.987080Z","shell.execute_reply":"2022-11-21T22:45:24.806449Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003937 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's rmse: 0.232565\tvalid_1's rmse: 0.593093\nEarly stopping, best iteration is:\n[107]\ttraining's rmse: 0.220856\tvalid_1's rmse: 0.592106\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's rmse: 0.231739\tvalid_1's rmse: 0.53191\nEarly stopping, best iteration is:\n[63]\ttraining's rmse: 0.308689\tvalid_1's rmse: 0.530372\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[38]\ttraining's rmse: 0.330625\tvalid_1's rmse: 0.503609\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003300 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's rmse: 0.226362\tvalid_1's rmse: 0.55456\nEarly stopping, best iteration is:\n[62]\ttraining's rmse: 0.305181\tvalid_1's rmse: 0.54948\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's rmse: 0.244494\tvalid_1's rmse: 0.596107\nEarly stopping, best iteration is:\n[144]\ttraining's rmse: 0.176415\tvalid_1's rmse: 0.593397\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[47]\ttraining's rmse: 0.367648\tvalid_1's rmse: 0.602823\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"              cohesion    syntax  vocabulary  phraseology   grammar  \\\ntext_id                                                               \n0000C359D63E  2.888183  2.985096    3.191583     2.987695  2.697193   \n000BAD50D026  3.003593  2.703766    2.883729     2.806867  2.647724   \n00367BB2546B  3.561003  3.514718    3.680797     3.526439  3.436347   \n\n              conventions  \ntext_id                    \n0000C359D63E     3.052911  \n000BAD50D026     3.158523  \n00367BB2546B     3.382326  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n    <tr>\n      <th>text_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0000C359D63E</th>\n      <td>2.888183</td>\n      <td>2.985096</td>\n      <td>3.191583</td>\n      <td>2.987695</td>\n      <td>2.697193</td>\n      <td>3.052911</td>\n    </tr>\n    <tr>\n      <th>000BAD50D026</th>\n      <td>3.003593</td>\n      <td>2.703766</td>\n      <td>2.883729</td>\n      <td>2.806867</td>\n      <td>2.647724</td>\n      <td>3.158523</td>\n    </tr>\n    <tr>\n      <th>00367BB2546B</th>\n      <td>3.561003</td>\n      <td>3.514718</td>\n      <td>3.680797</td>\n      <td>3.526439</td>\n      <td>3.436347</td>\n      <td>3.382326</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}