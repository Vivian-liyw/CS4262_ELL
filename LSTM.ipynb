{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m8CPXhk9X64Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch, gc\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import io\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading CSV\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t6ghpfcwFZx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b904450a-8793-4c56-f19d-cf7026306a72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/My Drive/MLProject/train.csv') \n",
        "df_test = pd.read_csv('/content/drive/My Drive/MLProject/test.csv')  "
      ],
      "metadata": {
        "id": "hGwRv-rGBM8m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine data set\n",
        "print(df_train.head())\n",
        "print(df_train['full_text'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSLMSliwvd5U",
        "outputId": "d5f1fc3a-ec58-4b40-bc2b-34195e5cafbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        text_id                                          full_text  cohesion  \\\n",
            "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
            "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
            "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
            "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
            "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
            "\n",
            "   syntax  vocabulary  phraseology  grammar  conventions  \n",
            "0     3.5         3.0          3.0      4.0          3.0  \n",
            "1     2.5         3.0          2.0      2.0          2.5  \n",
            "2     3.5         3.0          3.0      3.0          2.5  \n",
            "3     4.5         4.5          4.5      4.0          5.0  \n",
            "4     3.0         3.0          3.0      2.5          2.5  \n",
            "Dear, Principal\n",
            "\n",
            "If u change the school policy of having a grade b average that unfair. Because many students have a C average. So that means that they cant go out for sports or other activities they want to do bad. That's like taking everything they have. What if kids want to become good at something, but now they cant because of that school policy. If they have a C average they should still be able to go out for sports or activities. A C average isn't that bad, its higher then a D average. If the school police was if you have a D average of lower they shouldn't do sports or activities. If they have a D average in school for not working hard, that's means that they in ain't going to try hard. If they have a C average and there trying hard they should be able to out for sports or activities. What if all the good people in sports have a C average in school, that means that they cant play and were going to lose every game we have. That's a good policy to get grade's up but don't take away something they care about. Everyone should be able to go out for sports if they want to. If the school policy happens, schools going to be boarding now, because now students cant go out for sports or other activities. The students that are doing good in school should feel good about themselves but we shouldn't take the other students away from the others ones. If we do this policy student will try to raised their grade but if they cant what happens they them. Should they just be out of it and think that schools boarding. If they do this its like taking away their video games. All I'm saying is that they have the right to go out for sports or activities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data pre-processing\n",
        "# remove '\\n \\r \\w'\n",
        "df_train['full_text'] = df_train[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\n",
        "df_test['full_text'] = df_test[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\n",
        "df_train['full_text'] = df_train[\"full_text\"].replace(re.compile(r'[^\\w]'), ' ', regex=True)\n",
        "df_test['full_text'] = df_test[\"full_text\"].replace(re.compile(r'[^\\w]'), ' ', regex=True)\n",
        "\n",
        "# remove stop words\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "df_train['full_text'] = df_train['full_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "df_test['full_text'] = df_test['full_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "print(df_train['full_text'][2])\n",
        "\n",
        "# count max word length\n",
        "df_train['num_words'] = df_train['full_text'].apply(lambda x: len(x.split()))\n",
        "max_words = round(df_train['num_words'].max())\n",
        "print('max word:{}'.format(max_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lddG1zchv23d",
        "outputId": "70ffad98-3bed-46f4-b0b5-5322b916179d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Principal If u change school policy grade b average unfair Because many students C average So means cant go sports activities want bad That like taking everything What kids want become good something cant school policy If C average still able go sports activities A C average bad higher D average If school police D average lower sports activities If D average school working hard means going try hard If C average trying hard able sports activities What good people sports C average school means cant play going lose every game That good policy get grade take away something care Everyone able go sports want If school policy happens schools going boarding students cant go sports activities The students good school feel good take students away others ones If policy student try raised grade cant happens Should think schools boarding If like taking away video games All I saying right go sports activities\n",
            "max word:672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M1W4bebFhSq",
        "outputId": "80ee66c5-d6ac-4261-b3c6-50f0472ee4e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize words after tokenize\n",
        "train_token = df_train['full_text'].apply(word_tokenize)\n",
        "test_token = df_test['full_text'].apply(word_tokenize)\n",
        "\n",
        "print(train_token[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReLoPY-1HwvQ",
        "outputId": "6c2019fc-b33b-4755-ed70-cad9c53f57a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'think', 'students', 'would', 'benefit', 'learning', 'home', 'wont', 'change', 'get', 'early', 'morning', 'shower', 'hair', 'taking', 'classes', 'helps', 'house', 'pay', 'attention', 'comfortable', 'home', 'The', 'hardest', 'part', 'school', 'getting', 'ready', 'wake', 'go', 'brush', 'teeth', 'go', 'closet', 'look', 'cloths', 'think', 'picked', 'outfit', 'u', 'go', 'look', 'mirror', 'youll', 'either', 'like', 'look', 'see', 'stain', 'Then', 'change', 'online', 'classes', 'wear', 'anything', 'stay', 'home', 'wont', 'need', 'stress', 'wear', 'students', 'usually', 'take', 'showers', 'school', 'either', 'take', 'sleep', 'wake', 'students', 'smell', 'good', 'causes', 'miss', 'bus', 'effects', 'lesson', 'time', 'cause', 'come', 'late', 'school', 'u', 'online', 'classes', 'u', 'wont', 'need', 'miss', 'lessons', 'cause', 'get', 'everything', 'set', 'go', 'take', 'shower', 'u', 'get', 'ready', 'go', 'home', 'comfortable', 'pay', 'attention', 'gives', 'advantage', 'smarter', 'even', 'pass', 'classmates', 'class', 'work', 'public', 'schools', 'difficult', 'even', 'try', 'teacher', 'dont', 'know', 'teach', 'way', 'students', 'understand', 'causes', 'students', 'fail', 'may', 'repeat', 'class']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign x and y\n",
        "X = df_train['full_text']\n",
        "y = df_train[['cohesion','syntax','vocabulary','phraseology','grammar','conventions']]\n",
        "\n",
        "#split train test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
        "\n",
        "y_train = y_train.to_numpy()\n",
        "y_val = y_val.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "print(X_train[0])\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(X_val.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwXB-b3YFi_7",
        "outputId": "8755dd3b-9c25-4bf6-fb5e-13f908e53fb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think students would benefit learning home wont change get early morning shower hair taking classes helps house pay attention comfortable home The hardest part school getting ready wake go brush teeth go closet look cloths think picked outfit u go look mirror youll either like look see stain Then change online classes wear anything stay home wont need stress wear students usually take showers school either take sleep wake students smell good causes miss bus effects lesson time cause come late school u online classes u wont need miss lessons cause get everything set go take shower u get ready go home comfortable pay attention gives advantage smarter even pass classmates class work public schools difficult even try teacher dont know teach way students understand causes students fail may repeat class\n",
            "(2502,)\n",
            "(2502, 6)\n",
            "(626, 6)\n",
            "(626,)\n",
            "(783, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "EdUsuLVenhze"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "pad_train = pad_sequences(train_seq, maxlen=max_words, truncating='post')\n",
        "\n",
        "val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "pad_val = pad_sequences(val_seq, maxlen=max_words, truncating='post')\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "pad_test = pad_sequences(test_seq, maxlen=max_words, truncating='post') #max length of word is 1250\n",
        "\n",
        "print(pad_train[0]) #pad_train is a numpy array\n",
        "print(pad_train.shape)\n",
        "\n",
        "## for y, what should be the shape (6 lists each with its output? or m lists each with 6 outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2cgMUBPnzex",
        "outputId": "67fb2c1d-3153-4cf1-87da-7d59c7ed1c77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0   90   98  609 2270 4154 2358 1925  938  211 8565\n",
            "   33  113   84    3   16    5  846   71    8  538   57    4  164  116\n",
            "  835    2 2919   57    4    2  292    2   12  484    2   19    2   12\n",
            " 1091    4   32  186  846   71   35  250  117    2 1707   41  112   58\n",
            "  240    6   66   35   66 8566    2  200   66  170  156   33    2 1264\n",
            "    2   15   10 6305   60    7   78    5  406 3783  134  183  939  180\n",
            "  626  279  626 4632  145    5   57 3784  846   71   32   23  232  769\n",
            "  104   33   71   91  939  940    5   57    4    6  375   33  129   66\n",
            "   37   90    4    4 1707   41  115   41 1159   13    5  129   66   64\n",
            "    9 6306    5  626 2059 6307   18   91 2769  893  145   33    9  143\n",
            " 2359  198   91   19  640   57    4   60   63  846   12  165 1355  626\n",
            "  279  112   78   13   59   32    5   33   32 6308  112  599  826   86\n",
            " 3493    3   19 1850   18  469 1397 1771  778  110    6   25   66  145\n",
            "  626   32   32  279  264  836   21   52   46  264  112   71   21  196\n",
            "  366 6309  145  113   12 8567 2360  112    5  109   22 6310   31  600\n",
            "  104  626 4632    6    7   71   46   20   15   10   27  999 6305 2438\n",
            " 1030  470   12    2   15   10   70   24  633   35   11  941   52   62\n",
            "    6    7   71 1567  610   21 1992 2439  197  610   21  196 2440   86\n",
            "   34    5   57    4   21   19  164 1172    5 1134   33  113  161   24\n",
            "  232 2920    5  846   71   57    4    5  626 2059 6307   18   91 2769\n",
            "  893   32    5   33   32 6308  112  599  826   86 3493    3   19 1850\n",
            "   18  469 1397 1771  778  110    6   25   66  626  279   63   35   11\n",
            "  941   52   62    6    7   71  460  184   32    5 3785   71   57    4]\n",
            "(2502, 672)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_idx_count = len(word_index)\n",
        "print(word_idx_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjfdPi-gAdcq",
        "outputId": "06ea917a-880b-4e23-b925-a1dc43c83f42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# def data_sequence(input, label):\n",
        "#   inout_seq = []\n",
        "#   for i in range(input.shape[0]):\n",
        "#     seq = torch.Tensor(input[i,:])\n",
        "#     target = torch.Tensor(label[i,:])\n",
        "#     inout_seq.append((seq,target))\n",
        "#   return inout_seq\n",
        "\n",
        "# inout_seq = data_sequence(pad_train,y_train)\n",
        "# val_seq = data_sequence(pad_val,y_val)\n",
        "# print(inout_seq[3])\n",
        "\n",
        "batch_size = 1 # hyper parameter\n",
        "\n",
        "# load dataset\n",
        "trainX = torch.Tensor(pad_train)\n",
        "trainY = torch.Tensor(y_train)\n",
        "train = TensorDataset(trainX, trainY)\n",
        "\n",
        "valX = torch.Tensor(pad_val)\n",
        "valY = torch.Tensor(y_val)\n",
        "val = TensorDataset(valX,valY)\n",
        "\n",
        "testX = torch.Tensor(pad_test)\n",
        "testY = torch.Tensor(y_test)\n",
        "test = TensorDataset(testX,testY)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "val_loader = DataLoader(val, batch_size=1, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "tEl7ONAb_Pvt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSTM_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTM_Model, self).__init__()\n",
        "    self.embeddings = nn.Embedding(word_idx_count+1, 64)\n",
        "    self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, bidirectional=True, batch_first=True)\n",
        "    self.lstm2 = nn.LSTM(input_size=128, hidden_size=32, bidirectional=True, batch_first=True)\n",
        "    self.lstm3 = nn.LSTM(input_size=64, hidden_size=32, bidirectional=True, batch_first=True)\n",
        "    self.fc1 = torch.nn.Linear(64,64)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.fc2 = torch.nn.Linear(64,32)\n",
        "    self.fc3 = torch.nn.Linear(32,6)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    embeds = self.embeddings(inputs.to(torch.long))\n",
        "    h1, states = self.lstm1(embeds)\n",
        "    h2, states = self.lstm2(h1)  \n",
        "    h3, states = self.lstm3(h2)\n",
        "    h3 = h3[:,-1,:]\n",
        "    h4 = self.fc1(h3)\n",
        "    h4 = self.relu(h4)\n",
        "    h4 = self.fc2(h4)\n",
        "    h4 = self.relu(h4)\n",
        "    out = self.fc3(h4)\n",
        "    return out"
      ],
      "metadata": {
        "id": "lxXPj5vup6pn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkDBQETV5Rvg",
        "outputId": "cb52478b-caf7-453a-fbdf-7bec94c8ce6a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "epochs = 30  #hyper parameter\n",
        "model = LSTM_Model().to(device)\n",
        "print(model)\n",
        "print(summary(model,(1,672)))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # hyper parameter\n",
        "\n",
        "mse = nn.MSELoss()\n",
        "train_loss = []\n",
        "score = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
        "train_loss_score = [] # six score loss in each epoch\n",
        "val_loss_score = [] # six score loss in validation set \n",
        "best_val_mse = 10000\n",
        "     "
      ],
      "metadata": {
        "id": "2tPIqOhB9uwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77273bc1-aedb-4e7a-c44e-7d72749998c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM_Model(\n",
            "  (embeddings): Embedding(16807, 64)\n",
            "  (lstm1): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
            "  (lstm2): LSTM(128, 32, batch_first=True, bidirectional=True)\n",
            "  (lstm3): LSTM(64, 32, batch_first=True, bidirectional=True)\n",
            "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=6, bias=True)\n",
            ")\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "LSTM_Model                               [1, 6]                    --\n",
            "├─Embedding: 1-1                         [1, 672, 64]              1,075,648\n",
            "├─LSTM: 1-2                              [1, 672, 128]             66,560\n",
            "├─LSTM: 1-3                              [1, 672, 64]              41,472\n",
            "├─LSTM: 1-4                              [1, 672, 64]              25,088\n",
            "├─Linear: 1-5                            [1, 64]                   4,160\n",
            "├─ReLU: 1-6                              [1, 64]                   --\n",
            "├─Linear: 1-7                            [1, 32]                   2,080\n",
            "├─ReLU: 1-8                              [1, 32]                   --\n",
            "├─Linear: 1-9                            [1, 6]                    198\n",
            "==========================================================================================\n",
            "Total params: 1,215,206\n",
            "Trainable params: 1,215,206\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 90.54\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.72\n",
            "Params size (MB): 4.86\n",
            "Estimated Total Size (MB): 6.58\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, val_loader, epochs=100, val_epoch=20):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    batch_loss = []\n",
        "    for seq, labels in train_loader:\n",
        "      seq = seq.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # model training\n",
        "      model.train()\n",
        "      pred = model(seq)\n",
        "\n",
        "      # append each score into each list\n",
        "      loss_list = torch.mean(((pred - labels)**2),dim=0).tolist()\n",
        "      # print(loss_list)\n",
        "      batch_loss.append(loss_list)\n",
        "\n",
        "      # backward\n",
        "      loss = mse(pred, labels)\n",
        "      # print(f'mse loss {loss}')\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # #print batch loss\n",
        "      # for i,s in enumerate(loss_list):\n",
        "      #   print(f'batch loss {score[i]}: {loss_list[i]}')\n",
        "    \n",
        "    #append each epoch score loss\n",
        "    loss_epoch = np.mean(batch_loss, axis=0)\n",
        "    for i,s in enumerate(loss_epoch):\n",
        "      print(f'epoch {epoch} loss {score[i]}: {loss_epoch[i]}')\n",
        "    train_loss_score.append(loss_epoch)\n",
        "    train_loss.append(np.mean(loss_epoch))\n",
        "    print(f'total average loss in epoch {epoch}: {np.mean(loss_epoch)}')\n",
        "\n",
        "    # validation\n",
        "    if (epoch+1)%val_epoch == 0:\n",
        "      val_loss = []\n",
        "      with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "          x_val = x_val.to(device)\n",
        "          y_val = y_val.to(device)\n",
        "          model.eval()\n",
        "          yhat = model(x_val)\n",
        "          loss_val_list = ((yhat - y_val)**2).tolist()\n",
        "          val_loss.append(loss_val_list)\n",
        "      val_loss = np.mean(loss_val_list, axis=0)\n",
        "\n",
        "      # save best model\n",
        "      cur_mse = np.mean(val_epoch)\n",
        "      if(cur_mse < best_val_mse):\n",
        "        path = '/content/gdrive/My Drive/MLProject/model.pt'\n",
        "        torch.save(model.state_dict(),path)\n",
        "\n",
        "      #print validation mse\n",
        "      for i,s in enumerate(val_loss):\n",
        "        print(f'validation mse {score[i]}: {val_loss[i]}')\n",
        "      val_loss_score.append(val_loss)\n",
        "\n",
        "\n",
        "  return train_loss, train_loss_score, val_loss_score"
      ],
      "metadata": {
        "id": "xFn8Pciv2WGn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_loss_score, val_loss_score = train(train_loader,val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athLosItGsB9",
        "outputId": "ea3ee603-e8ba-486e-def1-935aaa30780e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 loss cohesion: 0.20922621414257164\n",
            "epoch 0 loss syntax: 0.17786084176124342\n",
            "epoch 0 loss vocabulary: 0.1638672492404549\n",
            "epoch 0 loss phraseology: 0.1774357223825032\n",
            "epoch 0 loss grammar: 0.20643337098936776\n",
            "epoch 0 loss conventions: 0.2015431896602571\n",
            "total average loss in epoch 0: 0.189394431362733\n",
            "epoch 1 loss cohesion: 0.2036153413111501\n",
            "epoch 1 loss syntax: 0.17623167793977423\n",
            "epoch 1 loss vocabulary: 0.15770355505916184\n",
            "epoch 1 loss phraseology: 0.1733844554538057\n",
            "epoch 1 loss grammar: 0.2046997413948322\n",
            "epoch 1 loss conventions: 0.20018512225289098\n",
            "total average loss in epoch 1: 0.18596998223526917\n",
            "epoch 2 loss cohesion: 0.18754723599023526\n",
            "epoch 2 loss syntax: 0.16074520062427317\n",
            "epoch 2 loss vocabulary: 0.1499366624911061\n",
            "epoch 2 loss phraseology: 0.16742164517181393\n",
            "epoch 2 loss grammar: 0.193929413721669\n",
            "epoch 2 loss conventions: 0.1861803940420459\n",
            "total average loss in epoch 2: 0.17429342534019054\n",
            "epoch 3 loss cohesion: 0.18274762951263088\n",
            "epoch 3 loss syntax: 0.1550369747123018\n",
            "epoch 3 loss vocabulary: 0.14699241426721713\n",
            "epoch 3 loss phraseology: 0.16413117977581387\n",
            "epoch 3 loss grammar: 0.19385580527959026\n",
            "epoch 3 loss conventions: 0.18589467225414488\n",
            "total average loss in epoch 3: 0.17144311263361645\n",
            "epoch 4 loss cohesion: 0.17809405351278662\n",
            "epoch 4 loss syntax: 0.14565379094484487\n",
            "epoch 4 loss vocabulary: 0.14205063531001066\n",
            "epoch 4 loss phraseology: 0.14737562225101764\n",
            "epoch 4 loss grammar: 0.18409389548087454\n",
            "epoch 4 loss conventions: 0.1794839859581578\n",
            "total average loss in epoch 4: 0.16279199724294868\n",
            "epoch 5 loss cohesion: 0.17275967120837116\n",
            "epoch 5 loss syntax: 0.14265714651626882\n",
            "epoch 5 loss vocabulary: 0.13659044737104944\n",
            "epoch 5 loss phraseology: 0.14369439973352505\n",
            "epoch 5 loss grammar: 0.17484942432908\n",
            "epoch 5 loss conventions: 0.16895830905787798\n",
            "total average loss in epoch 5: 0.15658489970269543\n",
            "epoch 6 loss cohesion: 0.16246067939942588\n",
            "epoch 6 loss syntax: 0.14084204087869837\n",
            "epoch 6 loss vocabulary: 0.13122367403823504\n",
            "epoch 6 loss phraseology: 0.139557453146688\n",
            "epoch 6 loss grammar: 0.16856190824137213\n",
            "epoch 6 loss conventions: 0.16640443385139753\n",
            "total average loss in epoch 6: 0.15150836492596945\n",
            "epoch 7 loss cohesion: 0.16524122228546378\n",
            "epoch 7 loss syntax: 0.1363132250817987\n",
            "epoch 7 loss vocabulary: 0.12384515903325877\n",
            "epoch 7 loss phraseology: 0.13712206841037736\n",
            "epoch 7 loss grammar: 0.16170938535193036\n",
            "epoch 7 loss conventions: 0.16176465468397186\n",
            "total average loss in epoch 7: 0.1476659524744668\n",
            "epoch 8 loss cohesion: 0.15860367030092337\n",
            "epoch 8 loss syntax: 0.13254013782591895\n",
            "epoch 8 loss vocabulary: 0.12524627126051774\n",
            "epoch 8 loss phraseology: 0.13233821482737582\n",
            "epoch 8 loss grammar: 0.16173258501291976\n",
            "epoch 8 loss conventions: 0.1657564702180674\n",
            "total average loss in epoch 8: 0.1460362249076205\n",
            "epoch 9 loss cohesion: 0.15563105120029194\n",
            "epoch 9 loss syntax: 0.1271213439199726\n",
            "epoch 9 loss vocabulary: 0.12580727312612514\n",
            "epoch 9 loss phraseology: 0.13130262246319155\n",
            "epoch 9 loss grammar: 0.15712250195184302\n",
            "epoch 9 loss conventions: 0.15867765302404735\n",
            "total average loss in epoch 9: 0.1426104076142453\n",
            "epoch 10 loss cohesion: 0.15193894620875498\n",
            "epoch 10 loss syntax: 0.12873177153189885\n",
            "epoch 10 loss vocabulary: 0.12335261282018746\n",
            "epoch 10 loss phraseology: 0.13092393555692794\n",
            "epoch 10 loss grammar: 0.16060731632813646\n",
            "epoch 10 loss conventions: 0.15509985322693265\n",
            "total average loss in epoch 10: 0.1417757392788064\n",
            "epoch 11 loss cohesion: 0.15447028356908532\n",
            "epoch 11 loss syntax: 0.12845869307387278\n",
            "epoch 11 loss vocabulary: 0.11547515440285451\n",
            "epoch 11 loss phraseology: 0.12711654488245624\n",
            "epoch 11 loss grammar: 0.15325750818565476\n",
            "epoch 11 loss conventions: 0.15427567709537568\n",
            "total average loss in epoch 11: 0.13884231020154988\n",
            "epoch 12 loss cohesion: 0.14853239935229293\n",
            "epoch 12 loss syntax: 0.12660547244030446\n",
            "epoch 12 loss vocabulary: 0.11670057581478852\n",
            "epoch 12 loss phraseology: 0.12814796004561535\n",
            "epoch 12 loss grammar: 0.14693694502984006\n",
            "epoch 12 loss conventions: 0.1497417591851079\n",
            "total average loss in epoch 12: 0.13611085197799153\n",
            "epoch 13 loss cohesion: 0.1485268658260852\n",
            "epoch 13 loss syntax: 0.1246822081409621\n",
            "epoch 13 loss vocabulary: 0.11774914761340877\n",
            "epoch 13 loss phraseology: 0.1237481918015516\n",
            "epoch 13 loss grammar: 0.14600433507472083\n",
            "epoch 13 loss conventions: 0.15253015580329127\n",
            "total average loss in epoch 13: 0.13554015071000328\n",
            "epoch 14 loss cohesion: 0.14976936331982124\n",
            "epoch 14 loss syntax: 0.1259406861590954\n",
            "epoch 14 loss vocabulary: 0.11620220773086447\n",
            "epoch 14 loss phraseology: 0.12405348224298948\n",
            "epoch 14 loss grammar: 0.14822659334487842\n",
            "epoch 14 loss conventions: 0.1518132099053656\n",
            "total average loss in epoch 14: 0.13600092378383577\n",
            "epoch 15 loss cohesion: 0.14989987040528344\n",
            "epoch 15 loss syntax: 0.12580011688281859\n",
            "epoch 15 loss vocabulary: 0.11802116597616516\n",
            "epoch 15 loss phraseology: 0.12491011000142291\n",
            "epoch 15 loss grammar: 0.14830244235339\n",
            "epoch 15 loss conventions: 0.15038508771842207\n",
            "total average loss in epoch 15: 0.1362197988895837\n",
            "epoch 16 loss cohesion: 0.1477156003096066\n",
            "epoch 16 loss syntax: 0.11798345365708586\n",
            "epoch 16 loss vocabulary: 0.11767383212819979\n",
            "epoch 16 loss phraseology: 0.12066052525548769\n",
            "epoch 16 loss grammar: 0.14539672083116206\n",
            "epoch 16 loss conventions: 0.1448205190247126\n",
            "total average loss in epoch 16: 0.13237510853437576\n",
            "epoch 17 loss cohesion: 0.14212365075108616\n",
            "epoch 17 loss syntax: 0.11421041646175227\n",
            "epoch 17 loss vocabulary: 0.11283735977200035\n",
            "epoch 17 loss phraseology: 0.11898519744959575\n",
            "epoch 17 loss grammar: 0.14520404593886924\n",
            "epoch 17 loss conventions: 0.15239435337517776\n",
            "total average loss in epoch 17: 0.1309591706247469\n",
            "epoch 18 loss cohesion: 0.14558028272830953\n",
            "epoch 18 loss syntax: 0.11808596289079829\n",
            "epoch 18 loss vocabulary: 0.11519274388114312\n",
            "epoch 18 loss phraseology: 0.1199918885296019\n",
            "epoch 18 loss grammar: 0.14437123266575166\n",
            "epoch 18 loss conventions: 0.1477106919082898\n",
            "total average loss in epoch 18: 0.1318221337673157\n",
            "epoch 19 loss cohesion: 0.1456359366874111\n",
            "epoch 19 loss syntax: 0.1193061533240188\n",
            "epoch 19 loss vocabulary: 0.11087798407242164\n",
            "epoch 19 loss phraseology: 0.11912246479283783\n",
            "epoch 19 loss grammar: 0.14116428132316566\n",
            "epoch 19 loss conventions: 0.14600084553256734\n",
            "total average loss in epoch 19: 0.13035127762207038\n",
            "validation mse cohesion: 0.1740598976612091\n",
            "validation mse syntax: 0.05958322808146477\n",
            "validation mse vocabulary: 0.24860529601573944\n",
            "validation mse phraseology: 0.40912601351737976\n",
            "validation mse grammar: 0.06576962769031525\n",
            "validation mse conventions: 0.11734853684902191\n",
            "epoch 20 loss cohesion: 0.1438557490922042\n",
            "epoch 20 loss syntax: 0.11912459745024631\n",
            "epoch 20 loss vocabulary: 0.11145226831021963\n",
            "epoch 20 loss phraseology: 0.12231084055559607\n",
            "epoch 20 loss grammar: 0.1418053845660097\n",
            "epoch 20 loss conventions: 0.14296631536543186\n",
            "total average loss in epoch 20: 0.1302525258899513\n",
            "epoch 21 loss cohesion: 0.14230179190230227\n",
            "epoch 21 loss syntax: 0.11913234506713496\n",
            "epoch 21 loss vocabulary: 0.11366659265015829\n",
            "epoch 21 loss phraseology: 0.11625483048677854\n",
            "epoch 21 loss grammar: 0.14204909363178217\n",
            "epoch 21 loss conventions: 0.14176817005558096\n",
            "total average loss in epoch 21: 0.12919547063228953\n",
            "epoch 22 loss cohesion: 0.14081195498035162\n",
            "epoch 22 loss syntax: 0.11652830072403104\n",
            "epoch 22 loss vocabulary: 0.11305646215963026\n",
            "epoch 22 loss phraseology: 0.11455461028469018\n",
            "epoch 22 loss grammar: 0.14295787686287212\n",
            "epoch 22 loss conventions: 0.14402912955684707\n",
            "total average loss in epoch 22: 0.12865638909473706\n",
            "epoch 23 loss cohesion: 0.14369690203720684\n",
            "epoch 23 loss syntax: 0.11439357092781803\n",
            "epoch 23 loss vocabulary: 0.11000378685128437\n",
            "epoch 23 loss phraseology: 0.11225020144852912\n",
            "epoch 23 loss grammar: 0.13982554823615567\n",
            "epoch 23 loss conventions: 0.1432676265697079\n",
            "total average loss in epoch 23: 0.12723960601178366\n",
            "epoch 24 loss cohesion: 0.1416534152435571\n",
            "epoch 24 loss syntax: 0.1152210268033349\n",
            "epoch 24 loss vocabulary: 0.11107862781334324\n",
            "epoch 24 loss phraseology: 0.11403345448867402\n",
            "epoch 24 loss grammar: 0.13385761612011196\n",
            "epoch 24 loss conventions: 0.14300618107881233\n",
            "total average loss in epoch 24: 0.1264750535913056\n",
            "epoch 25 loss cohesion: 0.1385667637210408\n",
            "epoch 25 loss syntax: 0.11140022463394375\n",
            "epoch 25 loss vocabulary: 0.10825232147168305\n",
            "epoch 25 loss phraseology: 0.11265353945914446\n",
            "epoch 25 loss grammar: 0.13387496811862296\n",
            "epoch 25 loss conventions: 0.1418916576329626\n",
            "total average loss in epoch 25: 0.12443991250623294\n",
            "epoch 26 loss cohesion: 0.13938908285541785\n",
            "epoch 26 loss syntax: 0.1131149941262918\n",
            "epoch 26 loss vocabulary: 0.11033556401897207\n",
            "epoch 26 loss phraseology: 0.11356924822784364\n",
            "epoch 26 loss grammar: 0.13282575609137148\n",
            "epoch 26 loss conventions: 0.13821650708913727\n",
            "total average loss in epoch 26: 0.12457519206817234\n",
            "epoch 27 loss cohesion: 0.14097512955647587\n",
            "epoch 27 loss syntax: 0.11559655578501321\n",
            "epoch 27 loss vocabulary: 0.11031315753890193\n",
            "epoch 27 loss phraseology: 0.1145988442870702\n",
            "epoch 27 loss grammar: 0.13582179946123402\n",
            "epoch 27 loss conventions: 0.14008679439596622\n",
            "total average loss in epoch 27: 0.1262320468374436\n",
            "epoch 28 loss cohesion: 0.1384170651275957\n",
            "epoch 28 loss syntax: 0.11364362841522449\n",
            "epoch 28 loss vocabulary: 0.10964969931282503\n",
            "epoch 28 loss phraseology: 0.1121774087126455\n",
            "epoch 28 loss grammar: 0.13547609225495272\n",
            "epoch 28 loss conventions: 0.14193873345956728\n",
            "total average loss in epoch 28: 0.1252171045471351\n",
            "epoch 29 loss cohesion: 0.13803214041532752\n",
            "epoch 29 loss syntax: 0.11030576664568444\n",
            "epoch 29 loss vocabulary: 0.10777835904057369\n",
            "epoch 29 loss phraseology: 0.107530126437778\n",
            "epoch 29 loss grammar: 0.13481161175245554\n",
            "epoch 29 loss conventions: 0.14127740595700836\n",
            "total average loss in epoch 29: 0.12328923504147125\n",
            "epoch 30 loss cohesion: 0.13455094006906845\n",
            "epoch 30 loss syntax: 0.11249584185596222\n",
            "epoch 30 loss vocabulary: 0.10627977832293675\n",
            "epoch 30 loss phraseology: 0.1089421233930217\n",
            "epoch 30 loss grammar: 0.13077500689104002\n",
            "epoch 30 loss conventions: 0.136289287916419\n",
            "total average loss in epoch 30: 0.12155549640807468\n",
            "epoch 31 loss cohesion: 0.1400729080443577\n",
            "epoch 31 loss syntax: 0.11290979448954938\n",
            "epoch 31 loss vocabulary: 0.10602736678913105\n",
            "epoch 31 loss phraseology: 0.11043016110940182\n",
            "epoch 31 loss grammar: 0.12787454935340753\n",
            "epoch 31 loss conventions: 0.1374598573597687\n",
            "total average loss in epoch 31: 0.12246243952426938\n",
            "epoch 32 loss cohesion: 0.13657700087054386\n",
            "epoch 32 loss syntax: 0.11195021802259576\n",
            "epoch 32 loss vocabulary: 0.10606917562863902\n",
            "epoch 32 loss phraseology: 0.1086579435715845\n",
            "epoch 32 loss grammar: 0.13105058261507022\n",
            "epoch 32 loss conventions: 0.13524203670546844\n",
            "total average loss in epoch 32: 0.12159115956898363\n",
            "epoch 33 loss cohesion: 0.13821884360114003\n",
            "epoch 33 loss syntax: 0.10888752575188587\n",
            "epoch 33 loss vocabulary: 0.10788044097278801\n",
            "epoch 33 loss phraseology: 0.11220179010393165\n",
            "epoch 33 loss grammar: 0.13309228900496645\n",
            "epoch 33 loss conventions: 0.13459884723024831\n",
            "total average loss in epoch 33: 0.12247995611082672\n",
            "epoch 34 loss cohesion: 0.13519954941651088\n",
            "epoch 34 loss syntax: 0.10955154052269969\n",
            "epoch 34 loss vocabulary: 0.10418502241150907\n",
            "epoch 34 loss phraseology: 0.10620692100498785\n",
            "epoch 34 loss grammar: 0.13240619916485538\n",
            "epoch 34 loss conventions: 0.13662970903319555\n",
            "total average loss in epoch 34: 0.12069649025895975\n",
            "epoch 35 loss cohesion: 0.13281811963885176\n",
            "epoch 35 loss syntax: 0.10749683322622983\n",
            "epoch 35 loss vocabulary: 0.1024185389477673\n",
            "epoch 35 loss phraseology: 0.10466513927385843\n",
            "epoch 35 loss grammar: 0.12663048368780633\n",
            "epoch 35 loss conventions: 0.1329069928714395\n",
            "total average loss in epoch 35: 0.11782268460765884\n",
            "epoch 36 loss cohesion: 0.13330325183479005\n",
            "epoch 36 loss syntax: 0.10761020053380804\n",
            "epoch 36 loss vocabulary: 0.09991299932399003\n",
            "epoch 36 loss phraseology: 0.10263148801463876\n",
            "epoch 36 loss grammar: 0.1259364797949117\n",
            "epoch 36 loss conventions: 0.13264450874062064\n",
            "total average loss in epoch 36: 0.11700648804045988\n",
            "epoch 37 loss cohesion: 0.1333391093230712\n",
            "epoch 37 loss syntax: 0.10881270221936776\n",
            "epoch 37 loss vocabulary: 0.10075815707256504\n",
            "epoch 37 loss phraseology: 0.10355092399222343\n",
            "epoch 37 loss grammar: 0.1254948763743807\n",
            "epoch 37 loss conventions: 0.1300722284364437\n",
            "total average loss in epoch 37: 0.11700466623634198\n",
            "epoch 38 loss cohesion: 0.13393480913662184\n",
            "epoch 38 loss syntax: 0.10779758226155643\n",
            "epoch 38 loss vocabulary: 0.10098109603201175\n",
            "epoch 38 loss phraseology: 0.10612965486815949\n",
            "epoch 38 loss grammar: 0.12662592109892942\n",
            "epoch 38 loss conventions: 0.1291991504723643\n",
            "total average loss in epoch 38: 0.11744470231160721\n",
            "epoch 39 loss cohesion: 0.1348136177197335\n",
            "epoch 39 loss syntax: 0.10760502821688964\n",
            "epoch 39 loss vocabulary: 0.09953411988380582\n",
            "epoch 39 loss phraseology: 0.100135436665036\n",
            "epoch 39 loss grammar: 0.1247762451736887\n",
            "epoch 39 loss conventions: 0.13005586667602656\n",
            "total average loss in epoch 39: 0.11615338572253003\n",
            "validation mse cohesion: 0.32131779193878174\n",
            "validation mse syntax: 0.16310201585292816\n",
            "validation mse vocabulary: 0.3957120180130005\n",
            "validation mse phraseology: 0.24463582038879395\n",
            "validation mse grammar: 0.1646946221590042\n",
            "validation mse conventions: 0.2550135552883148\n",
            "epoch 40 loss cohesion: 0.1330191119863015\n",
            "epoch 40 loss syntax: 0.10602477213074159\n",
            "epoch 40 loss vocabulary: 0.09720101305665421\n",
            "epoch 40 loss phraseology: 0.09692356501433735\n",
            "epoch 40 loss grammar: 0.12336423656342796\n",
            "epoch 40 loss conventions: 0.1310750176470963\n",
            "total average loss in epoch 40: 0.11460128606642649\n",
            "epoch 41 loss cohesion: 0.12910914906299406\n",
            "epoch 41 loss syntax: 0.10341749288131306\n",
            "epoch 41 loss vocabulary: 0.09700714212625745\n",
            "epoch 41 loss phraseology: 0.0984756432151386\n",
            "epoch 41 loss grammar: 0.12284893775492328\n",
            "epoch 41 loss conventions: 0.12992320655123119\n",
            "total average loss in epoch 41: 0.11346359526530962\n",
            "epoch 42 loss cohesion: 0.12910935918973587\n",
            "epoch 42 loss syntax: 0.10456934265457349\n",
            "epoch 42 loss vocabulary: 0.09877291755599497\n",
            "epoch 42 loss phraseology: 0.0995241614252235\n",
            "epoch 42 loss grammar: 0.12381095843527129\n",
            "epoch 42 loss conventions: 0.12482389042728569\n",
            "total average loss in epoch 42: 0.11343510494801413\n",
            "epoch 43 loss cohesion: 0.1299422608372894\n",
            "epoch 43 loss syntax: 0.10508166908247032\n",
            "epoch 43 loss vocabulary: 0.09725182970376293\n",
            "epoch 43 loss phraseology: 0.09638963061939987\n",
            "epoch 43 loss grammar: 0.12134200639485698\n",
            "epoch 43 loss conventions: 0.12422693525561088\n",
            "total average loss in epoch 43: 0.11237238864889841\n",
            "epoch 44 loss cohesion: 0.1268099577070031\n",
            "epoch 44 loss syntax: 0.10465129821396767\n",
            "epoch 44 loss vocabulary: 0.09675909360511199\n",
            "epoch 44 loss phraseology: 0.09537811348123511\n",
            "epoch 44 loss grammar: 0.11876147261700759\n",
            "epoch 44 loss conventions: 0.1226391144043147\n",
            "total average loss in epoch 44: 0.11083317500477335\n",
            "epoch 45 loss cohesion: 0.11692302591624895\n",
            "epoch 45 loss syntax: 0.10362599851997276\n",
            "epoch 45 loss vocabulary: 0.09627121329641126\n",
            "epoch 45 loss phraseology: 0.09251416260828835\n",
            "epoch 45 loss grammar: 0.11476422609816142\n",
            "epoch 45 loss conventions: 0.12449233043338538\n",
            "total average loss in epoch 45: 0.108098492812078\n",
            "epoch 46 loss cohesion: 0.10017473441226736\n",
            "epoch 46 loss syntax: 0.10483575389947608\n",
            "epoch 46 loss vocabulary: 0.09545813390977183\n",
            "epoch 46 loss phraseology: 0.09214475833322246\n",
            "epoch 46 loss grammar: 0.10816952910000613\n",
            "epoch 46 loss conventions: 0.12260389618248646\n",
            "total average loss in epoch 46: 0.10389780097287171\n",
            "epoch 47 loss cohesion: 0.08289442065589937\n",
            "epoch 47 loss syntax: 0.10339214069255827\n",
            "epoch 47 loss vocabulary: 0.09507097780959672\n",
            "epoch 47 loss phraseology: 0.08878824144267017\n",
            "epoch 47 loss grammar: 0.09963713555986463\n",
            "epoch 47 loss conventions: 0.11841599053810382\n",
            "total average loss in epoch 47: 0.09803315111644884\n",
            "epoch 48 loss cohesion: 0.07415613766432745\n",
            "epoch 48 loss syntax: 0.10272846561498228\n",
            "epoch 48 loss vocabulary: 0.0934781516631648\n",
            "epoch 48 loss phraseology: 0.08580162069494432\n",
            "epoch 48 loss grammar: 0.0928279556824668\n",
            "epoch 48 loss conventions: 0.11632582490201251\n",
            "total average loss in epoch 48: 0.0942196927036497\n",
            "epoch 49 loss cohesion: 0.06777276087721576\n",
            "epoch 49 loss syntax: 0.10096889109232225\n",
            "epoch 49 loss vocabulary: 0.09213840318796315\n",
            "epoch 49 loss phraseology: 0.08458622831144913\n",
            "epoch 49 loss grammar: 0.08770709099585292\n",
            "epoch 49 loss conventions: 0.11400300235281999\n",
            "total average loss in epoch 49: 0.0911960628029372\n",
            "epoch 50 loss cohesion: 0.06723274914697655\n",
            "epoch 50 loss syntax: 0.10225394882743295\n",
            "epoch 50 loss vocabulary: 0.09065954597307808\n",
            "epoch 50 loss phraseology: 0.08476347336888551\n",
            "epoch 50 loss grammar: 0.08434320428542486\n",
            "epoch 50 loss conventions: 0.11254880225411643\n",
            "total average loss in epoch 50: 0.09030028730931906\n",
            "epoch 51 loss cohesion: 0.061231448671373825\n",
            "epoch 51 loss syntax: 0.10392971358085437\n",
            "epoch 51 loss vocabulary: 0.08839976032129092\n",
            "epoch 51 loss phraseology: 0.08387196887281066\n",
            "epoch 51 loss grammar: 0.07705637744181255\n",
            "epoch 51 loss conventions: 0.10935238098598486\n",
            "total average loss in epoch 51: 0.08730694164568786\n",
            "epoch 52 loss cohesion: 0.063513328096426\n",
            "epoch 52 loss syntax: 0.10162344782811698\n",
            "epoch 52 loss vocabulary: 0.0839702462393227\n",
            "epoch 52 loss phraseology: 0.08263602786492365\n",
            "epoch 52 loss grammar: 0.07144813495327366\n",
            "epoch 52 loss conventions: 0.10731659383892271\n",
            "total average loss in epoch 52: 0.08508462980349762\n",
            "epoch 53 loss cohesion: 0.06070335848090522\n",
            "epoch 53 loss syntax: 0.10218571381390106\n",
            "epoch 53 loss vocabulary: 0.07878363387813923\n",
            "epoch 53 loss phraseology: 0.0847411260283386\n",
            "epoch 53 loss grammar: 0.06915935634057882\n",
            "epoch 53 loss conventions: 0.1066909509060384\n",
            "total average loss in epoch 53: 0.08371068990798357\n",
            "epoch 54 loss cohesion: 0.058647233042766284\n",
            "epoch 54 loss syntax: 0.10013657797708836\n",
            "epoch 54 loss vocabulary: 0.07349524430761374\n",
            "epoch 54 loss phraseology: 0.08204960181763497\n",
            "epoch 54 loss grammar: 0.0627311201008491\n",
            "epoch 54 loss conventions: 0.10430581027595562\n",
            "total average loss in epoch 54: 0.08022759792031801\n",
            "epoch 55 loss cohesion: 0.058568769537474814\n",
            "epoch 55 loss syntax: 0.09960336862885223\n",
            "epoch 55 loss vocabulary: 0.06979582055879928\n",
            "epoch 55 loss phraseology: 0.07954533839470956\n",
            "epoch 55 loss grammar: 0.05847259090035211\n",
            "epoch 55 loss conventions: 0.10146451370736614\n",
            "total average loss in epoch 55: 0.07790840028792569\n",
            "epoch 56 loss cohesion: 0.060396159448419195\n",
            "epoch 56 loss syntax: 0.10208719479140439\n",
            "epoch 56 loss vocabulary: 0.06780437907818755\n",
            "epoch 56 loss phraseology: 0.08039337968100717\n",
            "epoch 56 loss grammar: 0.056072978926589544\n",
            "epoch 56 loss conventions: 0.10029027883515348\n",
            "total average loss in epoch 56: 0.07784072846012689\n",
            "epoch 57 loss cohesion: 0.057230151005240024\n",
            "epoch 57 loss syntax: 0.09839347740041507\n",
            "epoch 57 loss vocabulary: 0.06344948909190389\n",
            "epoch 57 loss phraseology: 0.07863804539624693\n",
            "epoch 57 loss grammar: 0.05116534947992719\n",
            "epoch 57 loss conventions: 0.09573102103302689\n",
            "total average loss in epoch 57: 0.07410125556779333\n",
            "epoch 58 loss cohesion: 0.05803628344189416\n",
            "epoch 58 loss syntax: 0.09761387402718964\n",
            "epoch 58 loss vocabulary: 0.06331183378036336\n",
            "epoch 58 loss phraseology: 0.07571344047685065\n",
            "epoch 58 loss grammar: 0.048437218411362476\n",
            "epoch 58 loss conventions: 0.09565956103424543\n",
            "total average loss in epoch 58: 0.07312870186198428\n",
            "epoch 59 loss cohesion: 0.05693215443527959\n",
            "epoch 59 loss syntax: 0.09649517088192044\n",
            "epoch 59 loss vocabulary: 0.06318443369864357\n",
            "epoch 59 loss phraseology: 0.07316076308677505\n",
            "epoch 59 loss grammar: 0.04845918325443545\n",
            "epoch 59 loss conventions: 0.0930197532308504\n",
            "total average loss in epoch 59: 0.07187524309798408\n",
            "validation mse cohesion: 0.28157490491867065\n",
            "validation mse syntax: 0.014596734195947647\n",
            "validation mse vocabulary: 0.1028125137090683\n",
            "validation mse phraseology: 0.80022794008255\n",
            "validation mse grammar: 0.00011183181050000712\n",
            "validation mse conventions: 0.20414337515830994\n",
            "epoch 60 loss cohesion: 0.05643265424130776\n",
            "epoch 60 loss syntax: 0.09596307374999108\n",
            "epoch 60 loss vocabulary: 0.06335096708981063\n",
            "epoch 60 loss phraseology: 0.07121468289480831\n",
            "epoch 60 loss grammar: 0.04541631333633401\n",
            "epoch 60 loss conventions: 0.08890802637436765\n",
            "total average loss in epoch 60: 0.07021428628110322\n",
            "epoch 61 loss cohesion: 0.056348936752778474\n",
            "epoch 61 loss syntax: 0.09539502703673666\n",
            "epoch 61 loss vocabulary: 0.061267910736021655\n",
            "epoch 61 loss phraseology: 0.07055769510812403\n",
            "epoch 61 loss grammar: 0.04518602484116446\n",
            "epoch 61 loss conventions: 0.08586260315470756\n",
            "total average loss in epoch 61: 0.06910303293825547\n",
            "epoch 62 loss cohesion: 0.059255077861326555\n",
            "epoch 62 loss syntax: 0.09202804621057419\n",
            "epoch 62 loss vocabulary: 0.061389650110993\n",
            "epoch 62 loss phraseology: 0.07083796152101184\n",
            "epoch 62 loss grammar: 0.04325712747372503\n",
            "epoch 62 loss conventions: 0.08276775962101256\n",
            "total average loss in epoch 62: 0.0682559371331072\n",
            "epoch 63 loss cohesion: 0.05777402793460331\n",
            "epoch 63 loss syntax: 0.0916431106746587\n",
            "epoch 63 loss vocabulary: 0.06207566618126162\n",
            "epoch 63 loss phraseology: 0.0680537960297454\n",
            "epoch 63 loss grammar: 0.041531607652429275\n",
            "epoch 63 loss conventions: 0.07922117162119638\n",
            "total average loss in epoch 63: 0.06671656334898245\n",
            "epoch 64 loss cohesion: 0.058326551623412264\n",
            "epoch 64 loss syntax: 0.09025769034035523\n",
            "epoch 64 loss vocabulary: 0.06213655722112389\n",
            "epoch 64 loss phraseology: 0.0687018455276062\n",
            "epoch 64 loss grammar: 0.04047769764771066\n",
            "epoch 64 loss conventions: 0.07800504996632442\n",
            "total average loss in epoch 64: 0.06631756538775545\n",
            "epoch 65 loss cohesion: 0.05830775133169626\n",
            "epoch 65 loss syntax: 0.08970111725221128\n",
            "epoch 65 loss vocabulary: 0.061738021579504095\n",
            "epoch 65 loss phraseology: 0.06741140809155276\n",
            "epoch 65 loss grammar: 0.03961848918189834\n",
            "epoch 65 loss conventions: 0.07499720416966728\n",
            "total average loss in epoch 65: 0.065295665267755\n",
            "epoch 66 loss cohesion: 0.057952702208533054\n",
            "epoch 66 loss syntax: 0.08655318588570457\n",
            "epoch 66 loss vocabulary: 0.06295865596152019\n",
            "epoch 66 loss phraseology: 0.06602232722853972\n",
            "epoch 66 loss grammar: 0.03820939663378888\n",
            "epoch 66 loss conventions: 0.07122973213396964\n",
            "total average loss in epoch 66: 0.06382100000867601\n",
            "epoch 67 loss cohesion: 0.05889917362158343\n",
            "epoch 67 loss syntax: 0.08928159758343236\n",
            "epoch 67 loss vocabulary: 0.061454262177900024\n",
            "epoch 67 loss phraseology: 0.06644003788593339\n",
            "epoch 67 loss grammar: 0.03764057526063405\n",
            "epoch 67 loss conventions: 0.07002627142566309\n",
            "total average loss in epoch 67: 0.06395698632585772\n",
            "epoch 68 loss cohesion: 0.06002874432744224\n",
            "epoch 68 loss syntax: 0.08503977206620414\n",
            "epoch 68 loss vocabulary: 0.06391299445449539\n",
            "epoch 68 loss phraseology: 0.0654475815899028\n",
            "epoch 68 loss grammar: 0.035596119993244836\n",
            "epoch 68 loss conventions: 0.06710868395960813\n",
            "total average loss in epoch 68: 0.06285564939848293\n",
            "epoch 69 loss cohesion: 0.05839946878441949\n",
            "epoch 69 loss syntax: 0.08630029254375303\n",
            "epoch 69 loss vocabulary: 0.06143668020253132\n",
            "epoch 69 loss phraseology: 0.06469302022056779\n",
            "epoch 69 loss grammar: 0.036161508705053205\n",
            "epoch 69 loss conventions: 0.06621235449582587\n",
            "total average loss in epoch 69: 0.06220055415869178\n",
            "epoch 70 loss cohesion: 0.059986182944481395\n",
            "epoch 70 loss syntax: 0.08421308638847627\n",
            "epoch 70 loss vocabulary: 0.06243613978477202\n",
            "epoch 70 loss phraseology: 0.062316532020783307\n",
            "epoch 70 loss grammar: 0.03488814246829255\n",
            "epoch 70 loss conventions: 0.06181441513998269\n",
            "total average loss in epoch 70: 0.06094241645779804\n",
            "epoch 71 loss cohesion: 0.059995035944679545\n",
            "epoch 71 loss syntax: 0.0831926679272108\n",
            "epoch 71 loss vocabulary: 0.06131749716294819\n",
            "epoch 71 loss phraseology: 0.06210685798637353\n",
            "epoch 71 loss grammar: 0.03437605201508166\n",
            "epoch 71 loss conventions: 0.06013921104096098\n",
            "total average loss in epoch 71: 0.06018788701287578\n",
            "epoch 72 loss cohesion: 0.059627578765300004\n",
            "epoch 72 loss syntax: 0.08356835045833699\n",
            "epoch 72 loss vocabulary: 0.06429470146135939\n",
            "epoch 72 loss phraseology: 0.06173627314761528\n",
            "epoch 72 loss grammar: 0.03351633805175772\n",
            "epoch 72 loss conventions: 0.058600718270331545\n",
            "total average loss in epoch 72: 0.06022399335911682\n",
            "epoch 73 loss cohesion: 0.06012816502366758\n",
            "epoch 73 loss syntax: 0.08293117247689383\n",
            "epoch 73 loss vocabulary: 0.06404802071194407\n",
            "epoch 73 loss phraseology: 0.060552599326896155\n",
            "epoch 73 loss grammar: 0.03420292446979849\n",
            "epoch 73 loss conventions: 0.05785146016113495\n",
            "total average loss in epoch 73: 0.059952390361722514\n",
            "epoch 74 loss cohesion: 0.06110665943676734\n",
            "epoch 74 loss syntax: 0.08364418855776237\n",
            "epoch 74 loss vocabulary: 0.06408742031422149\n",
            "epoch 74 loss phraseology: 0.060933877148985675\n",
            "epoch 74 loss grammar: 0.033550040581309164\n",
            "epoch 74 loss conventions: 0.05563748006136318\n",
            "total average loss in epoch 74: 0.059826611016734865\n",
            "epoch 75 loss cohesion: 0.059889530212871134\n",
            "epoch 75 loss syntax: 0.0819262009890876\n",
            "epoch 75 loss vocabulary: 0.06323256662311053\n",
            "epoch 75 loss phraseology: 0.05948899705218034\n",
            "epoch 75 loss grammar: 0.03237724359232847\n",
            "epoch 75 loss conventions: 0.05515245696759232\n",
            "total average loss in epoch 75: 0.05867783257286172\n",
            "epoch 76 loss cohesion: 0.06066451741938097\n",
            "epoch 76 loss syntax: 0.08282252954256898\n",
            "epoch 76 loss vocabulary: 0.06242164909928284\n",
            "epoch 76 loss phraseology: 0.05912658338827768\n",
            "epoch 76 loss grammar: 0.030367547002268114\n",
            "epoch 76 loss conventions: 0.05305888051615924\n",
            "total average loss in epoch 76: 0.05807695116132298\n",
            "epoch 77 loss cohesion: 0.062104763835696505\n",
            "epoch 77 loss syntax: 0.08094466207091014\n",
            "epoch 77 loss vocabulary: 0.06302482418049468\n",
            "epoch 77 loss phraseology: 0.05933808037939344\n",
            "epoch 77 loss grammar: 0.030703070969464046\n",
            "epoch 77 loss conventions: 0.051203252778344276\n",
            "total average loss in epoch 77: 0.057886442369050516\n",
            "epoch 78 loss cohesion: 0.060734368636652734\n",
            "epoch 78 loss syntax: 0.07950500832829167\n",
            "epoch 78 loss vocabulary: 0.06401655017927964\n",
            "epoch 78 loss phraseology: 0.05686723101632036\n",
            "epoch 78 loss grammar: 0.03006611228162239\n",
            "epoch 78 loss conventions: 0.05123385398081703\n",
            "total average loss in epoch 78: 0.05707052073716396\n",
            "epoch 79 loss cohesion: 0.06027026030144948\n",
            "epoch 79 loss syntax: 0.07974180672458964\n",
            "epoch 79 loss vocabulary: 0.06243405493649881\n",
            "epoch 79 loss phraseology: 0.05705491190015079\n",
            "epoch 79 loss grammar: 0.02868845037678176\n",
            "epoch 79 loss conventions: 0.050327578098113196\n",
            "total average loss in epoch 79: 0.056419510389597284\n",
            "validation mse cohesion: 0.043701380491256714\n",
            "validation mse syntax: 0.05412570759654045\n",
            "validation mse vocabulary: 0.2981961965560913\n",
            "validation mse phraseology: 0.7195384502410889\n",
            "validation mse grammar: 0.2636690139770508\n",
            "validation mse conventions: 0.0638163760304451\n",
            "epoch 80 loss cohesion: 0.05970594453119727\n",
            "epoch 80 loss syntax: 0.0781410308693286\n",
            "epoch 80 loss vocabulary: 0.06304333814227878\n",
            "epoch 80 loss phraseology: 0.056581273714628275\n",
            "epoch 80 loss grammar: 0.029771508220386465\n",
            "epoch 80 loss conventions: 0.048581170735971034\n",
            "total average loss in epoch 80: 0.05597071103563173\n",
            "epoch 81 loss cohesion: 0.06025870093296703\n",
            "epoch 81 loss syntax: 0.07831242391826598\n",
            "epoch 81 loss vocabulary: 0.06205561126199934\n",
            "epoch 81 loss phraseology: 0.05591982932958535\n",
            "epoch 81 loss grammar: 0.02840853032123518\n",
            "epoch 81 loss conventions: 0.04838923650375099\n",
            "total average loss in epoch 81: 0.05555738871130065\n",
            "epoch 82 loss cohesion: 0.060496650485230466\n",
            "epoch 82 loss syntax: 0.08307712295913969\n",
            "epoch 82 loss vocabulary: 0.06509031794025835\n",
            "epoch 82 loss phraseology: 0.05544972564992766\n",
            "epoch 82 loss grammar: 0.028656623378611686\n",
            "epoch 82 loss conventions: 0.04959671034467978\n",
            "total average loss in epoch 82: 0.05706119179297461\n",
            "epoch 83 loss cohesion: 0.058892183418150584\n",
            "epoch 83 loss syntax: 0.07809908143602054\n",
            "epoch 83 loss vocabulary: 0.06294474607529071\n",
            "epoch 83 loss phraseology: 0.05317342532314237\n",
            "epoch 83 loss grammar: 0.027611638265169883\n",
            "epoch 83 loss conventions: 0.04747134461311725\n",
            "total average loss in epoch 83: 0.05469873652181522\n",
            "epoch 84 loss cohesion: 0.059215066863602256\n",
            "epoch 84 loss syntax: 0.07627329282712252\n",
            "epoch 84 loss vocabulary: 0.06320478850627338\n",
            "epoch 84 loss phraseology: 0.0551065275393222\n",
            "epoch 84 loss grammar: 0.02978672344581985\n",
            "epoch 84 loss conventions: 0.04843555913024486\n",
            "total average loss in epoch 84: 0.055336993052064166\n",
            "epoch 85 loss cohesion: 0.05918500381290655\n",
            "epoch 85 loss syntax: 0.07911281002306046\n",
            "epoch 85 loss vocabulary: 0.06083789370902062\n",
            "epoch 85 loss phraseology: 0.0533667025985773\n",
            "epoch 85 loss grammar: 0.028758892988852206\n",
            "epoch 85 loss conventions: 0.04585722232274157\n",
            "total average loss in epoch 85: 0.054519754242526454\n",
            "epoch 86 loss cohesion: 0.056804028146908256\n",
            "epoch 86 loss syntax: 0.07813601536861783\n",
            "epoch 86 loss vocabulary: 0.05921120226036647\n",
            "epoch 86 loss phraseology: 0.05259538499217581\n",
            "epoch 86 loss grammar: 0.026708879634003264\n",
            "epoch 86 loss conventions: 0.04440891573844106\n",
            "total average loss in epoch 86: 0.05297740435675211\n",
            "epoch 87 loss cohesion: 0.054983024457342516\n",
            "epoch 87 loss syntax: 0.07697314967055296\n",
            "epoch 87 loss vocabulary: 0.058031187312699746\n",
            "epoch 87 loss phraseology: 0.052157378851220654\n",
            "epoch 87 loss grammar: 0.026053021352633224\n",
            "epoch 87 loss conventions: 0.044081355574997344\n",
            "total average loss in epoch 87: 0.05204651953657441\n",
            "epoch 88 loss cohesion: 0.05670679654891188\n",
            "epoch 88 loss syntax: 0.07595747239320935\n",
            "epoch 88 loss vocabulary: 0.05919517564896769\n",
            "epoch 88 loss phraseology: 0.053167516411258955\n",
            "epoch 88 loss grammar: 0.02748720253657725\n",
            "epoch 88 loss conventions: 0.044614985701611924\n",
            "total average loss in epoch 88: 0.05285485820675618\n",
            "epoch 89 loss cohesion: 0.05481535686544346\n",
            "epoch 89 loss syntax: 0.07555167264757308\n",
            "epoch 89 loss vocabulary: 0.057412552191186204\n",
            "epoch 89 loss phraseology: 0.05274098308710289\n",
            "epoch 89 loss grammar: 0.026558990929653286\n",
            "epoch 89 loss conventions: 0.04448774250783795\n",
            "total average loss in epoch 89: 0.05192788303813281\n",
            "epoch 90 loss cohesion: 0.05290171242205658\n",
            "epoch 90 loss syntax: 0.07534012901600377\n",
            "epoch 90 loss vocabulary: 0.05746610074516718\n",
            "epoch 90 loss phraseology: 0.05242809536968769\n",
            "epoch 90 loss grammar: 0.026706387725098046\n",
            "epoch 90 loss conventions: 0.044440961054161725\n",
            "total average loss in epoch 90: 0.0515472310553625\n",
            "epoch 91 loss cohesion: 0.053850804304319946\n",
            "epoch 91 loss syntax: 0.07721227944985486\n",
            "epoch 91 loss vocabulary: 0.05708920362927355\n",
            "epoch 91 loss phraseology: 0.052099744637113116\n",
            "epoch 91 loss grammar: 0.024717632897810888\n",
            "epoch 91 loss conventions: 0.04414722777966621\n",
            "total average loss in epoch 91: 0.05151948211633977\n",
            "epoch 92 loss cohesion: 0.05265061341553914\n",
            "epoch 92 loss syntax: 0.07591616024588903\n",
            "epoch 92 loss vocabulary: 0.05679778074371212\n",
            "epoch 92 loss phraseology: 0.052745564474866266\n",
            "epoch 92 loss grammar: 0.027070465394485144\n",
            "epoch 92 loss conventions: 0.04448968385228826\n",
            "total average loss in epoch 92: 0.051611711354463324\n",
            "epoch 93 loss cohesion: 0.05251106963312049\n",
            "epoch 93 loss syntax: 0.0756564014448999\n",
            "epoch 93 loss vocabulary: 0.055620081030214444\n",
            "epoch 93 loss phraseology: 0.05157975650705129\n",
            "epoch 93 loss grammar: 0.026390947333901005\n",
            "epoch 93 loss conventions: 0.04320145831430035\n",
            "total average loss in epoch 93: 0.05082661904391458\n",
            "epoch 94 loss cohesion: 0.05159425926041015\n",
            "epoch 94 loss syntax: 0.07375572432964794\n",
            "epoch 94 loss vocabulary: 0.055471620694269363\n",
            "epoch 94 loss phraseology: 0.051581896753828266\n",
            "epoch 94 loss grammar: 0.025821073477844576\n",
            "epoch 94 loss conventions: 0.04172461351988244\n",
            "total average loss in epoch 94: 0.049991531339313784\n",
            "epoch 95 loss cohesion: 0.05269313256893646\n",
            "epoch 95 loss syntax: 0.07448777595469988\n",
            "epoch 95 loss vocabulary: 0.05593433886664975\n",
            "epoch 95 loss phraseology: 0.05283509574864233\n",
            "epoch 95 loss grammar: 0.025957060983878653\n",
            "epoch 95 loss conventions: 0.0420094259981413\n",
            "total average loss in epoch 95: 0.050652805020158065\n",
            "epoch 96 loss cohesion: 0.05129914717444825\n",
            "epoch 96 loss syntax: 0.07478373851568997\n",
            "epoch 96 loss vocabulary: 0.05500966054879529\n",
            "epoch 96 loss phraseology: 0.05207141669569692\n",
            "epoch 96 loss grammar: 0.026927629777762488\n",
            "epoch 96 loss conventions: 0.04361042310501202\n",
            "total average loss in epoch 96: 0.05061700263623415\n",
            "epoch 97 loss cohesion: 0.052163936844037015\n",
            "epoch 97 loss syntax: 0.07346271804241741\n",
            "epoch 97 loss vocabulary: 0.054420052293072375\n",
            "epoch 97 loss phraseology: 0.050978515113405974\n",
            "epoch 97 loss grammar: 0.027465992231996948\n",
            "epoch 97 loss conventions: 0.03971323789761618\n",
            "total average loss in epoch 97: 0.04970074207042432\n",
            "epoch 98 loss cohesion: 0.05012090996692177\n",
            "epoch 98 loss syntax: 0.07349647184244756\n",
            "epoch 98 loss vocabulary: 0.05429257166550102\n",
            "epoch 98 loss phraseology: 0.05172318709307626\n",
            "epoch 98 loss grammar: 0.02619641637886248\n",
            "epoch 98 loss conventions: 0.039709445751200806\n",
            "total average loss in epoch 98: 0.04925650044966832\n",
            "epoch 99 loss cohesion: 0.05090296986415908\n",
            "epoch 99 loss syntax: 0.07314505298774736\n",
            "epoch 99 loss vocabulary: 0.054652772642483284\n",
            "epoch 99 loss phraseology: 0.050832307857534204\n",
            "epoch 99 loss grammar: 0.025381961922011996\n",
            "epoch 99 loss conventions: 0.03837418115017281\n",
            "total average loss in epoch 99: 0.04888154107068479\n",
            "validation mse cohesion: 0.1466306746006012\n",
            "validation mse syntax: 2.7339909138390794e-05\n",
            "validation mse vocabulary: 0.14652606844902039\n",
            "validation mse phraseology: 0.6745838522911072\n",
            "validation mse grammar: 0.03095925971865654\n",
            "validation mse conventions: 0.021512538194656372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loss)\n",
        "print(train_loss_score)\n",
        "print(val_loss_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waOJSS-oU5Wn",
        "outputId": "10a9412b-d350-48a2-fceb-e6d0a4e0f4f3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6082704822262234, 0.4389886433115186, 0.3948004810179661, 0.33876251826106346, 0.28687198412800147, 0.24626520540410035, 0.21719584816114568, 0.19726087188283892, 0.189394431362733, 0.18596998223526917, 0.17429342534019054, 0.17144311263361645, 0.16279199724294868, 0.15658489970269543, 0.15150836492596945, 0.1476659524744668, 0.1460362249076205, 0.1426104076142453, 0.1417757392788064, 0.13884231020154988, 0.13611085197799153, 0.13554015071000328, 0.13600092378383577, 0.1362197988895837, 0.13237510853437576, 0.1309591706247469, 0.1318221337673157, 0.13035127762207038, 0.1302525258899513, 0.12919547063228953, 0.12865638909473706, 0.12723960601178366, 0.1264750535913056, 0.12443991250623294, 0.12457519206817234, 0.1262320468374436, 0.1252171045471351, 0.12328923504147125, 0.12155549640807468, 0.12246243952426938, 0.12159115956898363, 0.12247995611082672, 0.12069649025895975, 0.11782268460765884, 0.11700648804045988, 0.11700466623634198, 0.11744470231160721, 0.11615338572253003, 0.11460128606642649, 0.11346359526530962, 0.11343510494801413, 0.11237238864889841, 0.11083317500477335, 0.108098492812078, 0.10389780097287171, 0.09803315111644884, 0.0942196927036497, 0.0911960628029372, 0.09030028730931906, 0.08730694164568786, 0.08508462980349762, 0.08371068990798357, 0.08022759792031801, 0.07790840028792569, 0.07784072846012689, 0.07410125556779333, 0.07312870186198428, 0.07187524309798408, 0.07021428628110322, 0.06910303293825547, 0.0682559371331072, 0.06671656334898245, 0.06631756538775545, 0.065295665267755, 0.06382100000867601, 0.06395698632585772, 0.06285564939848293, 0.06220055415869178, 0.06094241645779804, 0.06018788701287578, 0.06022399335911682, 0.059952390361722514, 0.059826611016734865, 0.05867783257286172, 0.05807695116132298, 0.057886442369050516, 0.05707052073716396, 0.056419510389597284, 0.05597071103563173, 0.05555738871130065, 0.05706119179297461, 0.05469873652181522, 0.055336993052064166, 0.054519754242526454, 0.05297740435675211, 0.05204651953657441, 0.05285485820675618, 0.05192788303813281, 0.0515472310553625, 0.05151948211633977, 0.051611711354463324, 0.05082661904391458, 0.049991531339313784, 0.050652805020158065, 0.05061700263623415, 0.04970074207042432, 0.04925650044966832, 0.04888154107068479]\n",
            "[array([0.63017863, 0.56864504, 0.51069171, 0.62709683, 0.68293843,\n",
            "       0.63007225]), array([0.44801482, 0.43114798, 0.35494811, 0.43381303, 0.5013618 ,\n",
            "       0.46464612]), array([0.40566396, 0.39154898, 0.31610666, 0.39112264, 0.4462693 ,\n",
            "       0.41809135]), array([0.35198013, 0.33431246, 0.275262  , 0.33467294, 0.37883134,\n",
            "       0.35751625]), array([0.3035876 , 0.2795109 , 0.2346943 , 0.28188987, 0.32004816,\n",
            "       0.30150107]), array([0.2654919 , 0.23795118, 0.20487311, 0.23710954, 0.27359372,\n",
            "       0.25857179]), array([0.23408649, 0.20541032, 0.18560254, 0.2102178 , 0.24286986,\n",
            "       0.22498807]), array([0.21553581, 0.18311709, 0.17043276, 0.1880834 , 0.21705928,\n",
            "       0.20933689]), array([0.20922621, 0.17786084, 0.16386725, 0.17743572, 0.20643337,\n",
            "       0.20154319]), array([0.20361534, 0.17623168, 0.15770356, 0.17338446, 0.20469974,\n",
            "       0.20018512]), array([0.18754724, 0.1607452 , 0.14993666, 0.16742165, 0.19392941,\n",
            "       0.18618039]), array([0.18274763, 0.15503697, 0.14699241, 0.16413118, 0.19385581,\n",
            "       0.18589467]), array([0.17809405, 0.14565379, 0.14205064, 0.14737562, 0.1840939 ,\n",
            "       0.17948399]), array([0.17275967, 0.14265715, 0.13659045, 0.1436944 , 0.17484942,\n",
            "       0.16895831]), array([0.16246068, 0.14084204, 0.13122367, 0.13955745, 0.16856191,\n",
            "       0.16640443]), array([0.16524122, 0.13631323, 0.12384516, 0.13712207, 0.16170939,\n",
            "       0.16176465]), array([0.15860367, 0.13254014, 0.12524627, 0.13233821, 0.16173259,\n",
            "       0.16575647]), array([0.15563105, 0.12712134, 0.12580727, 0.13130262, 0.1571225 ,\n",
            "       0.15867765]), array([0.15193895, 0.12873177, 0.12335261, 0.13092394, 0.16060732,\n",
            "       0.15509985]), array([0.15447028, 0.12845869, 0.11547515, 0.12711654, 0.15325751,\n",
            "       0.15427568]), array([0.1485324 , 0.12660547, 0.11670058, 0.12814796, 0.14693695,\n",
            "       0.14974176]), array([0.14852687, 0.12468221, 0.11774915, 0.12374819, 0.14600434,\n",
            "       0.15253016]), array([0.14976936, 0.12594069, 0.11620221, 0.12405348, 0.14822659,\n",
            "       0.15181321]), array([0.14989987, 0.12580012, 0.11802117, 0.12491011, 0.14830244,\n",
            "       0.15038509]), array([0.1477156 , 0.11798345, 0.11767383, 0.12066053, 0.14539672,\n",
            "       0.14482052]), array([0.14212365, 0.11421042, 0.11283736, 0.1189852 , 0.14520405,\n",
            "       0.15239435]), array([0.14558028, 0.11808596, 0.11519274, 0.11999189, 0.14437123,\n",
            "       0.14771069]), array([0.14563594, 0.11930615, 0.11087798, 0.11912246, 0.14116428,\n",
            "       0.14600085]), array([0.14385575, 0.1191246 , 0.11145227, 0.12231084, 0.14180538,\n",
            "       0.14296632]), array([0.14230179, 0.11913235, 0.11366659, 0.11625483, 0.14204909,\n",
            "       0.14176817]), array([0.14081195, 0.1165283 , 0.11305646, 0.11455461, 0.14295788,\n",
            "       0.14402913]), array([0.1436969 , 0.11439357, 0.11000379, 0.1122502 , 0.13982555,\n",
            "       0.14326763]), array([0.14165342, 0.11522103, 0.11107863, 0.11403345, 0.13385762,\n",
            "       0.14300618]), array([0.13856676, 0.11140022, 0.10825232, 0.11265354, 0.13387497,\n",
            "       0.14189166]), array([0.13938908, 0.11311499, 0.11033556, 0.11356925, 0.13282576,\n",
            "       0.13821651]), array([0.14097513, 0.11559656, 0.11031316, 0.11459884, 0.1358218 ,\n",
            "       0.14008679]), array([0.13841707, 0.11364363, 0.1096497 , 0.11217741, 0.13547609,\n",
            "       0.14193873]), array([0.13803214, 0.11030577, 0.10777836, 0.10753013, 0.13481161,\n",
            "       0.14127741]), array([0.13455094, 0.11249584, 0.10627978, 0.10894212, 0.13077501,\n",
            "       0.13628929]), array([0.14007291, 0.11290979, 0.10602737, 0.11043016, 0.12787455,\n",
            "       0.13745986]), array([0.136577  , 0.11195022, 0.10606918, 0.10865794, 0.13105058,\n",
            "       0.13524204]), array([0.13821884, 0.10888753, 0.10788044, 0.11220179, 0.13309229,\n",
            "       0.13459885]), array([0.13519955, 0.10955154, 0.10418502, 0.10620692, 0.1324062 ,\n",
            "       0.13662971]), array([0.13281812, 0.10749683, 0.10241854, 0.10466514, 0.12663048,\n",
            "       0.13290699]), array([0.13330325, 0.1076102 , 0.099913  , 0.10263149, 0.12593648,\n",
            "       0.13264451]), array([0.13333911, 0.1088127 , 0.10075816, 0.10355092, 0.12549488,\n",
            "       0.13007223]), array([0.13393481, 0.10779758, 0.1009811 , 0.10612965, 0.12662592,\n",
            "       0.12919915]), array([0.13481362, 0.10760503, 0.09953412, 0.10013544, 0.12477625,\n",
            "       0.13005587]), array([0.13301911, 0.10602477, 0.09720101, 0.09692357, 0.12336424,\n",
            "       0.13107502]), array([0.12910915, 0.10341749, 0.09700714, 0.09847564, 0.12284894,\n",
            "       0.12992321]), array([0.12910936, 0.10456934, 0.09877292, 0.09952416, 0.12381096,\n",
            "       0.12482389]), array([0.12994226, 0.10508167, 0.09725183, 0.09638963, 0.12134201,\n",
            "       0.12422694]), array([0.12680996, 0.1046513 , 0.09675909, 0.09537811, 0.11876147,\n",
            "       0.12263911]), array([0.11692303, 0.103626  , 0.09627121, 0.09251416, 0.11476423,\n",
            "       0.12449233]), array([0.10017473, 0.10483575, 0.09545813, 0.09214476, 0.10816953,\n",
            "       0.1226039 ]), array([0.08289442, 0.10339214, 0.09507098, 0.08878824, 0.09963714,\n",
            "       0.11841599]), array([0.07415614, 0.10272847, 0.09347815, 0.08580162, 0.09282796,\n",
            "       0.11632582]), array([0.06777276, 0.10096889, 0.0921384 , 0.08458623, 0.08770709,\n",
            "       0.114003  ]), array([0.06723275, 0.10225395, 0.09065955, 0.08476347, 0.0843432 ,\n",
            "       0.1125488 ]), array([0.06123145, 0.10392971, 0.08839976, 0.08387197, 0.07705638,\n",
            "       0.10935238]), array([0.06351333, 0.10162345, 0.08397025, 0.08263603, 0.07144813,\n",
            "       0.10731659]), array([0.06070336, 0.10218571, 0.07878363, 0.08474113, 0.06915936,\n",
            "       0.10669095]), array([0.05864723, 0.10013658, 0.07349524, 0.0820496 , 0.06273112,\n",
            "       0.10430581]), array([0.05856877, 0.09960337, 0.06979582, 0.07954534, 0.05847259,\n",
            "       0.10146451]), array([0.06039616, 0.10208719, 0.06780438, 0.08039338, 0.05607298,\n",
            "       0.10029028]), array([0.05723015, 0.09839348, 0.06344949, 0.07863805, 0.05116535,\n",
            "       0.09573102]), array([0.05803628, 0.09761387, 0.06331183, 0.07571344, 0.04843722,\n",
            "       0.09565956]), array([0.05693215, 0.09649517, 0.06318443, 0.07316076, 0.04845918,\n",
            "       0.09301975]), array([0.05643265, 0.09596307, 0.06335097, 0.07121468, 0.04541631,\n",
            "       0.08890803]), array([0.05634894, 0.09539503, 0.06126791, 0.0705577 , 0.04518602,\n",
            "       0.0858626 ]), array([0.05925508, 0.09202805, 0.06138965, 0.07083796, 0.04325713,\n",
            "       0.08276776]), array([0.05777403, 0.09164311, 0.06207567, 0.0680538 , 0.04153161,\n",
            "       0.07922117]), array([0.05832655, 0.09025769, 0.06213656, 0.06870185, 0.0404777 ,\n",
            "       0.07800505]), array([0.05830775, 0.08970112, 0.06173802, 0.06741141, 0.03961849,\n",
            "       0.0749972 ]), array([0.0579527 , 0.08655319, 0.06295866, 0.06602233, 0.0382094 ,\n",
            "       0.07122973]), array([0.05889917, 0.0892816 , 0.06145426, 0.06644004, 0.03764058,\n",
            "       0.07002627]), array([0.06002874, 0.08503977, 0.06391299, 0.06544758, 0.03559612,\n",
            "       0.06710868]), array([0.05839947, 0.08630029, 0.06143668, 0.06469302, 0.03616151,\n",
            "       0.06621235]), array([0.05998618, 0.08421309, 0.06243614, 0.06231653, 0.03488814,\n",
            "       0.06181442]), array([0.05999504, 0.08319267, 0.0613175 , 0.06210686, 0.03437605,\n",
            "       0.06013921]), array([0.05962758, 0.08356835, 0.0642947 , 0.06173627, 0.03351634,\n",
            "       0.05860072]), array([0.06012817, 0.08293117, 0.06404802, 0.0605526 , 0.03420292,\n",
            "       0.05785146]), array([0.06110666, 0.08364419, 0.06408742, 0.06093388, 0.03355004,\n",
            "       0.05563748]), array([0.05988953, 0.0819262 , 0.06323257, 0.059489  , 0.03237724,\n",
            "       0.05515246]), array([0.06066452, 0.08282253, 0.06242165, 0.05912658, 0.03036755,\n",
            "       0.05305888]), array([0.06210476, 0.08094466, 0.06302482, 0.05933808, 0.03070307,\n",
            "       0.05120325]), array([0.06073437, 0.07950501, 0.06401655, 0.05686723, 0.03006611,\n",
            "       0.05123385]), array([0.06027026, 0.07974181, 0.06243405, 0.05705491, 0.02868845,\n",
            "       0.05032758]), array([0.05970594, 0.07814103, 0.06304334, 0.05658127, 0.02977151,\n",
            "       0.04858117]), array([0.0602587 , 0.07831242, 0.06205561, 0.05591983, 0.02840853,\n",
            "       0.04838924]), array([0.06049665, 0.08307712, 0.06509032, 0.05544973, 0.02865662,\n",
            "       0.04959671]), array([0.05889218, 0.07809908, 0.06294475, 0.05317343, 0.02761164,\n",
            "       0.04747134]), array([0.05921507, 0.07627329, 0.06320479, 0.05510653, 0.02978672,\n",
            "       0.04843556]), array([0.059185  , 0.07911281, 0.06083789, 0.0533667 , 0.02875889,\n",
            "       0.04585722]), array([0.05680403, 0.07813602, 0.0592112 , 0.05259538, 0.02670888,\n",
            "       0.04440892]), array([0.05498302, 0.07697315, 0.05803119, 0.05215738, 0.02605302,\n",
            "       0.04408136]), array([0.0567068 , 0.07595747, 0.05919518, 0.05316752, 0.0274872 ,\n",
            "       0.04461499]), array([0.05481536, 0.07555167, 0.05741255, 0.05274098, 0.02655899,\n",
            "       0.04448774]), array([0.05290171, 0.07534013, 0.0574661 , 0.0524281 , 0.02670639,\n",
            "       0.04444096]), array([0.0538508 , 0.07721228, 0.0570892 , 0.05209974, 0.02471763,\n",
            "       0.04414723]), array([0.05265061, 0.07591616, 0.05679778, 0.05274556, 0.02707047,\n",
            "       0.04448968]), array([0.05251107, 0.0756564 , 0.05562008, 0.05157976, 0.02639095,\n",
            "       0.04320146]), array([0.05159426, 0.07375572, 0.05547162, 0.0515819 , 0.02582107,\n",
            "       0.04172461]), array([0.05269313, 0.07448778, 0.05593434, 0.0528351 , 0.02595706,\n",
            "       0.04200943]), array([0.05129915, 0.07478374, 0.05500966, 0.05207142, 0.02692763,\n",
            "       0.04361042]), array([0.05216394, 0.07346272, 0.05442005, 0.05097852, 0.02746599,\n",
            "       0.03971324]), array([0.05012091, 0.07349647, 0.05429257, 0.05172319, 0.02619642,\n",
            "       0.03970945]), array([0.05090297, 0.07314505, 0.05465277, 0.05083231, 0.02538196,\n",
            "       0.03837418])]\n",
            "[array([0.17172547, 0.09572944, 0.23995657, 0.32387605, 0.09406459,\n",
            "       0.17810366]), array([0.19903882, 0.10586739, 0.27305636, 0.2900658 , 0.12007111,\n",
            "       0.19739036]), array([0.1740599 , 0.05958323, 0.2486053 , 0.40912601, 0.06576963,\n",
            "       0.11734854]), array([0.32131779, 0.16310202, 0.39571202, 0.24463582, 0.16469462,\n",
            "       0.25501356]), array([2.81574905e-01, 1.45967342e-02, 1.02812514e-01, 8.00227940e-01,\n",
            "       1.11831811e-04, 2.04143375e-01]), array([0.04370138, 0.05412571, 0.2981962 , 0.71953845, 0.26366901,\n",
            "       0.06381638]), array([1.46630675e-01, 2.73399091e-05, 1.46526068e-01, 6.74583852e-01,\n",
            "       3.09592597e-02, 2.15125382e-02])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "path = '/content/drive/My Drive/MLProject/model.pt'\n",
        "torch.save(model.state_dict(),path)"
      ],
      "metadata": {
        "id": "QcsKYfDogHvz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "path = '/content/drive/My Drive/MLProject/model.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcvYjNJQgM4r",
        "outputId": "e195c64f-8ee8-4b8a-d835-0a084c2a2b42"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "val_loss_score = np.array(val_loss_score)\n",
        "plt.plot(val_loss_score[:,0],color='r',label='cohesion')\n",
        "plt.plot(val_loss_score[:,1],color='g',label='syntax')\n",
        "plt.plot(val_loss_score[:,2],color='b',label='vocabulary')\n",
        "plt.plot(val_loss_score[:,3],color='y',label='phraseology')\n",
        "plt.plot(val_loss_score[:,4],color='purple',label='grammar')\n",
        "plt.plot(val_loss_score[:,5],color='orange',label='convetions')\n",
        "plt.legend()\n",
        "plt.xlabel('validation step')\n",
        "plt.ylabel('mse')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "vaWkji1OgmL_",
        "outputId": "1df77ecb-944e-41be-dbac-1618297b92b7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = np.array(train_loss)\n",
        "plt.plot(train_loss,color='r',label='avg_mse')\n",
        "# plt.plot(train_loss[:,0],color='r',label='cohesion')\n",
        "# plt.plot(train_loss[:,1],color='g',label='syntax')\n",
        "# plt.plot(train_loss[:,2],color='b',label='vocabulary')\n",
        "# plt.plot(train_loss[:,3],color='y',label='phraseology')\n",
        "# plt.plot(train_loss[:,4],color='purple',label='grammar')\n",
        "# plt.plot(train_loss[:,5],color='orange',label='convetions')\n",
        "plt.legend()\n",
        "plt.xlabel('train step')\n",
        "plt.ylabel('avg_mse')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nWUmfm3RkOvc",
        "outputId": "28bf41ed-6212-4a31-c7da-f9c5b27f11d1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test set\n",
        "\n",
        "test_X = df_test['full_text'].to_numpy()\n",
        "x = tokenizer.texts_to_sequences(test_X)\n",
        "x = pad_sequences(x, maxlen=max_words, truncating='post')\n",
        "\n",
        "x = torch.Tensor(x)\n",
        "x = TensorDataset(x)\n",
        "testloader = DataLoader(x, batch_size=1, shuffle=False, drop_last=True)\n",
        "\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "  for sub in testloader:\n",
        "    sub = sub[0].to(device)\n",
        "    model.eval()\n",
        "    yhat = model(sub).tolist()[0]\n",
        "    results.append(yhat)\n",
        "\n",
        "lstm_submission = pd.DataFrame(results, columns=['Cohesion','Syntax','Vocabulary','Phraseology','Grammar','Convention'])\n",
        "lstm_submission.insert(0, 'text_id', df_test['text_id'])\n",
        "lstm_submission.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "diPzh1jLhLQ7",
        "outputId": "05016aa3-6e1f-4da6-e771-aadb186d3842"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        text_id  Cohesion    Syntax  Vocabulary  Phraseology   Grammar  \\\n",
              "0  0000C359D63E  3.312129  3.451136    3.296302     3.354572  4.156092   \n",
              "1  000BAD50D026  3.817288  3.426240    3.557919     3.456287  3.281771   \n",
              "2  00367BB2546B  3.425822  3.372378    3.783271     3.662610  3.358083   \n",
              "\n",
              "   Convention  \n",
              "0    3.915102  \n",
              "1    3.793662  \n",
              "2    3.152497  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a942311-4a4e-4b9e-94ea-25f151ea691d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>Cohesion</th>\n",
              "      <th>Syntax</th>\n",
              "      <th>Vocabulary</th>\n",
              "      <th>Phraseology</th>\n",
              "      <th>Grammar</th>\n",
              "      <th>Convention</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.312129</td>\n",
              "      <td>3.451136</td>\n",
              "      <td>3.296302</td>\n",
              "      <td>3.354572</td>\n",
              "      <td>4.156092</td>\n",
              "      <td>3.915102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>3.817288</td>\n",
              "      <td>3.426240</td>\n",
              "      <td>3.557919</td>\n",
              "      <td>3.456287</td>\n",
              "      <td>3.281771</td>\n",
              "      <td>3.793662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.425822</td>\n",
              "      <td>3.372378</td>\n",
              "      <td>3.783271</td>\n",
              "      <td>3.662610</td>\n",
              "      <td>3.358083</td>\n",
              "      <td>3.152497</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a942311-4a4e-4b9e-94ea-25f151ea691d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a942311-4a4e-4b9e-94ea-25f151ea691d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a942311-4a4e-4b9e-94ea-25f151ea691d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_submission.to_csv('submission.csv')\n",
        "!cp submission.csv '/content/drive/My Drive/MLProject/'"
      ],
      "metadata": {
        "id": "6r3QBG4Xt5WB"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}